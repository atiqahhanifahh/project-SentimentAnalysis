from http import HTTPStatus
from django.http import Http404
from rest_framework.response import Response
from rest_framework.views import APIView
from rest_framework.generics import get_object_or_404
from rest_framework.permissions import IsAuthenticatedOrReadOnly

from api.serializers import *
import json
import os
import numpy as np

def DataAniesBaswedan():
    """Sentiment Analysis Anies Baswedan_TF-IDF, NB, SVM.ipynb

    Automatically generated by Colaboratory.

    Original file is located at
        https://colab.research.google.com/drive/1cQsWcB2sX-MCyFSriMjqvEdcz0SIck_0

    # About

    Sentiment analysis pada dataset twitter Anies Baswedan menggunakan TF-IDF, Naive Bayes, dan Support Vector Machine
    """

    import numpy as np
    import pandas as pd

    """# Dataset"""

    # dataset_ab = 'data/Anies_Labeling_3000.csv'
    dataset_ab = 'static/datasets_anies.csv'

    data_anies = pd.read_csv(dataset_ab, delimiter=';')
    data_anies

    data_anies.isnull().sum()

    df_anies = data_anies.copy()

    """mapping label yang berbentuk angka menjadi bentuk klasifikasi (positif, negatif, neutral)"""

    df_anies["observasi"] = df_anies["label"].map({0: "negatif", 1: "positif", 2: "neutral"})
    df_anies.head()

    # Cek sample dari data
    sample_anies = df_anies.groupby('label').apply(lambda x: x.sample(5))
    sample_anies

    df_anies["observasi"].value_counts()

    """Hitung proporsi distribusi untuk menentukan metode OVR atau OVO dalam model Naive Bayes"""

    # Tentukan distribusi masing-masing hasil labeling
    class_distribution = pd.DataFrame({'sentimen': ['positif', 'negatif', 'neutral'],
                                    'value_counts': [1398, 1119, 483]})

    # Hitung proporsi distribusi
    class_distribution['proporsi'] = (class_distribution['value_counts'] / class_distribution['value_counts'].sum()) * 100
    print(class_distribution)

    """# TF IDF dan Support Vector Machine"""

    def feature_extraction(data, method="tfidf"):
        from sklearn.feature_extraction.text import TfidfVectorizer

        feature_extraction = TfidfVectorizer(sublinear_tf=True)
        features = feature_extraction.fit_transform(data)

        return features

    def svm_classifier(features, label, classifier="svm"):
        from sklearn.metrics import roc_auc_score
        from sklearn.svm import SVC
        from sklearn.preprocessing import label_binarize

        model = SVC(probability=True)
        model.fit(features, label)
        probability_to_be_positive = model.predict_proba(features)
        predict_output = model.predict(features)

        # Konversi label multi-class ke format one-hot-encoding
        binarized_label = label_binarize(label, classes=np.unique(label))

        # Gunakan  multi_class='ovo' karena proporsi distribusi tidak seimbang
        print("auc (train data):" , roc_auc_score(
            binarized_label,
            probability_to_be_positive,
            multi_class='ovo'))

        print("top 10 scores:")
        for i in range(10):
            if predict_output[i] == 0:
                label = 'negatif'
            elif predict_output[i] == 1:
                label = 'positif'
            elif predict_output[i] == 2:
                label = 'neutral'
            print(f"{probability_to_be_positive[i]} = {predict_output[i]} ({label})")

        return model

    data = np.array(df_anies["tweet"])
    label = np.array(df_anies["label"])

    """Lakukan pembobotan untuk masing-masing kata pada tweet."""

    features = feature_extraction(data, method="tfidf")
    print(features)

    """Lakukan training menggunakan model Naive Bayes."""

    svm_train = svm_classifier(features, label, "naive_bayes")
    print(svm_train)

    import pandas as pd
    import numpy as np
    import pickle

    from sklearn.svm import SVC
    from sklearn.metrics import classification_report
    from sklearn.model_selection import train_test_split
    from sklearn.feature_extraction.text import CountVectorizer
    from sklearn.feature_extraction.text import TfidfTransformer
    from sklearn.pipeline import Pipeline

    """Lakukan pemrosesan TF-IDF dengan data tweet sebagai variabel X dan data label sebagai variabel y."""

    X = df_anies['tweet']
    y = df_anies['label']

    bow_transformer = CountVectorizer().fit(df_anies['tweet'])
    messages_bow = bow_transformer.transform(df_anies['tweet'])
    tfidf_transformer = TfidfTransformer().fit(messages_bow)
    messages_tfidf = tfidf_transformer.transform(messages_bow)

    """Lakukan kembali proses train dan split dengan rasio yang sama yaitu data train sebesar 80% dan data test sebesar 20%."""

    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)

    pipeline = Pipeline([
        ('bow',CountVectorizer()),
        ('tfidf',TfidfTransformer()),
        ('classifier',SVC())
        ])

    X_train = np.asarray(X)
    pipeline = pipeline.fit(X_train, np.asarray(y))

    """Buat data pickle setelah proses training selesai dilakukan. Pickle akan digunakan untuk menjalankan proses deep learning."""

    pickle_name = 'df_anies_svm.pickle'
    pickle_path = 'data/'

    with open(pickle_path+pickle_name, 'wb') as f:
        pickle.dump(pipeline, f)

    """Ganti parameter open untuk membaca atau menggunakan pickle."""

    with open(pickle_path+pickle_name, 'rb') as f:
        pickle_anies_svm = pickle.load(f)
        print(pickle_anies_svm)

    """Lakukan prediksi dengan dataset lain"""

    path_test = 'data/Anies_Testing_Model.csv'

    df_test = pd.read_csv(path_test, delimiter=',')
    df_test.head()

    """Berdasarkan dataset, data yang diperlukan pada feature enineering ini hanyalah data terakhir yaitu setelah pemrosesan pada tahap pre-processing selesai dilakukan."""

    df_test = df_test[["stemming"]].rename(columns={"stemming": "tweet"})
    df_test = df_test["tweet"]
    df_test = df_anies["tweet"]
    df_test

    null_values = df_test.isnull().sum()
    print(f"Total null values: {null_values}")

    duplikat = df_test.duplicated(keep=False).sum()
    print("Total duplikat:", duplikat)

    # Drop duplikat jika ada
    # df_test = df_test.drop_duplicates()

    duplikat = df_test.duplicated(keep=False).sum()
    print("Total duplikat:", duplikat)

    """Lakukan prediksi menggunakan pickle yang telah dibuat sebelumnya."""

    prediction = pickle_anies_svm.predict(np.asarray(df_test))
    prediction

    """Simpan hasil prediksi ke dalam dataset dengan cara melakukan looping sesuai hasil prediksi. Parameter yang digunakan masih sama seperti sebelumnya, yaitu:

    - Prediksi 0: Negatif
    - Prediksi 1: Positif
    - Prediksi 2: Neutral
    """

    result = []

    for i in range(len(prediction)):
        if(prediction[i] == 0):
            sentiment = 'negatif'
        elif(prediction[i] == 1):
            sentiment = 'positif'
        elif(prediction[i] == 2):
            sentiment = 'neutral'

        result.append({'tweet':df_test.iloc[i],'label':prediction[i],'prediksi':sentiment})

    data = pd.DataFrame(result)
    data

    sample_prediksi = data.groupby('prediksi').apply(lambda x: x.sample(5))
    sample_prediksi

    """Lakukan pengecekan tiap data pada masing-masing prediksi, kemudian buat wordcloudnya"""

    import nltk
    nltk.download('punkt')

    """Lakukan pengecekan pada data positif."""

    positif = data.loc[data['prediksi']=='positif', 'tweet']
    positif

    """Lakukan looping untuk melihat setiap tweet dengan sentimen positif"""

    for pos in positif[:10]:
        print(pos)

    from wordcloud import WordCloud
    import matplotlib.pyplot as plt

    # Tokenize ulang kolom "NoStopwordsTweets" agar kalimat dikonversi menjadi kata-kata.
    re_tokenize = [nltk.word_tokenize(tweet) for tweet in positif]

    # Flatten: Konversi list kata-kata ke dalam satu list
    list_kata = nltk.FreqDist([kata for tweet in re_tokenize for kata in tweet])

    # Wordcloud
    wordcloud = WordCloud(width=800, height=400, colormap='Greens').generate_from_frequencies(list_kata)
    # plt.figure(figsize=(10, 6))
    # plt.imshow(wordcloud, interpolation='bilinear')
    # plt.axis('off')
    # plt.show()

    top_10_positif = list_kata.most_common(10)

    data_positif = {
        'Kata Positif': [kata for kata, total in top_10_positif],
        'Total': [total for kata, total in top_10_positif]
    }

    tabel_positif = pd.DataFrame(data_positif)
    tabel_positif

    """Lakukan pengecekan pada data negatif."""

    negatif = data.loc[data['prediksi']=='negatif', 'tweet']
    negatif

    """Lakukan looping untuk melihat setiap tweet dengan sentimen negatif"""

    for neg in negatif[:10]:
        print(neg)

    from wordcloud import WordCloud
    import matplotlib.pyplot as plt

    # Tokenize ulang kolom "NoStopwordsTweets" agar kalimat dikonversi menjadi kata-kata.
    re_tokenize = [nltk.word_tokenize(tweet) for tweet in negatif]

    # Flatten: Konversi list kata-kata ke dalam satu list
    list_kata = nltk.FreqDist([kata for tweet in re_tokenize for kata in tweet])

    # Wordcloud
    wordcloud = WordCloud(width=800, height=400, colormap='hot').generate_from_frequencies(list_kata)
    # plt.figure(figsize=(10, 6))
    # plt.imshow(wordcloud, interpolation='bilinear')
    # plt.axis('off')
    # plt.show()

    top_10_negatif = list_kata.most_common(10)

    data_negatif = {
        'Kata Negatif': [kata for kata, total in top_10_negatif],
        'Total': [total for kata, total in top_10_negatif]
    }

    tabel_negatif = pd.DataFrame(data_negatif)
    tabel_negatif

    """Lakukan pengecekan pada data neutral."""

    neutral = data.loc[data['prediksi']=='neutral', 'tweet']
    neutral

    for net in neutral[:10]:
        print(net)

    from wordcloud import WordCloud
    import matplotlib.pyplot as plt

    # Tokenize ulang kolom "NoStopwordsTweets" agar kalimat dikonversi menjadi kata-kata.
    re_tokenize = [nltk.word_tokenize(tweet) for tweet in neutral]

    # Flatten: Konversi list kata-kata ke dalam satu list
    list_kata = nltk.FreqDist([kata for tweet in re_tokenize for kata in tweet])

    # Wordcloud
    wordcloud = WordCloud(width=800, height=400, colormap='Blues_r').generate_from_frequencies(list_kata)
    # plt.figure(figsize=(10, 6))
    # plt.imshow(wordcloud, interpolation='bilinear')
    # plt.axis('off')
    # plt.show()

    top_10_neutral = list_kata.most_common(10)

    data_neutral = {
        'Kata Neutral': [kata for kata, total in top_10_neutral],
        'Total': [total for kata, total in top_10_neutral]
    }

    tabel_neutral = pd.DataFrame(data_neutral)
    tabel_neutral

    """Lanjutkan dengan melihat banyaknya sentiment positif, negatif, dan netral dari hasil prediksi."""

    data.groupby(by='prediksi').agg('count')

    data.groupby(by='prediksi')['tweet'].agg('count')

    """Tampilkan dalam bentuk grafik untuk memudahkan dalam melihat klasifikasi sentimen."""

    list_prediksi = data.groupby(by='prediksi').agg('count').values.tolist()
    print(list_prediksi)

    n_negatif = list_prediksi[0][0]
    n_neutral = list_prediksi[1][0]
    n_positif = list_prediksi[2][0]

    import matplotlib.pyplot as plt
    import seaborn as sns

    # plt.rcParams["figure.figsize"] = [8,5]
    # plt.rcParams["figure.autolayout"] = True

    x = ['Positif', 'Negatif', 'Neutral']
    y = [n_positif, n_negatif, n_neutral]
    percentage = [n_positif, n_negatif, n_neutral]

    # ax = sns.barplot(x=x, y=y)
    # patches = ax.patches
    # for i in range(len(patches)):
    #     x = patches[i].get_x() + patches[i].get_width()/2
    #     y = patches[i].get_height()+50
    #     ax.annotate('{:}'.format(percentage[i]), (x, y), ha='center')

    # plt.title('Klasifikasi Sentiment \n')
    # plt.xlabel('Jenis Klasifikasi')
    # plt.ylabel('Jumlah')
    # plt.show()

    """Berdasarkan grafik, didapatkan data sebagai berikut.

    1. Sentiment positif merupakan sentiment terbanyak dengan total 5420 sentiment.
    2. Sentiment negatif merupakan sentiment dengan total 4877 sentiment.
    3. Sentiment neutral merupakan sentiment tersedikit dengan total 1110 sentiment.

    ## Evaluasi

    Cek confussion matrix, classification report, dan cross val score.
    """

    data2 = data.copy()
    data2.head(5)

    """Tentukan variabel X dan y terlebih dahulu agar dapat digunakan pada pemodelan."""

    # Menentukan x berdasarkan label
    data_tweet = pd.DataFrame(data2,columns=['tweet'])
    X = data_tweet['tweet']
    X

    # Menentukan y berdasarkan label
    data_label = pd.DataFrame(data2,columns=['label'])
    y = data_label['label']
    y

    """Lakukan pembagian antara data train dan data test dengan rasio 8:2 atau data train sebesar 80% dan data test sebesar 20%."""

    from sklearn.model_selection import train_test_split

    # Split dataset untuk train dan test
    X_train,X_test,Y_train,Y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    """Lakukan vektorisasi untuk TF-IDF."""

    from sklearn.feature_extraction.text import TfidfVectorizer

    # TF-IDF train-test
    vect = TfidfVectorizer(analyzer="word", min_df=0.0004, max_df=0.115, ngram_range=(1,3))
    vect.fit(X_train)
    X_train_dtm = vect.transform(X_train)
    X_test_dtm = vect.transform(X_test)

    """Lakukan perhitungan skor akurasi berdasarkan MultinomialNB."""

    from sklearn.naive_bayes import MultinomialNB
    from sklearn.metrics import accuracy_score

    # Melakukan perhitungan skor akurasi berdasarkan MultinomialNB
    svmmodel = SVC(gamma=0.1)
    svmmodel = svmmodel.fit(X_train_dtm,Y_train)
    Y_pred = svmmodel.predict(X_test_dtm)
    print(f"Akurasi: {accuracy_score(Y_test,Y_pred)*100:.2f}")

    """Tentukan hasil dari confusion matrix"""

    from sklearn.metrics import confusion_matrix

    # Menentukan confusion matrix
    cm = confusion_matrix(Y_test, Y_pred)
    print(f"Confusion Matrix:\n\n{cm}")

    sentimen = ['Negatif', 'Positif', 'Neutral']
    df_cm = pd.DataFrame(cm, index=sentimen, columns=sentimen)
    df_cm

    """Tentukan hasil laporan klasifikasi."""

    from sklearn.metrics import classification_report

    # Menentukan laporan hasil klasifikasi
    report = classification_report(Y_test, Y_pred)
    print(f"Hasil Laporan Klasifikasi:\n\n{report}")

    """Tentukan nilai cross_val_score."""

    from sklearn.model_selection import cross_val_score

    # Menentukan nilai cross_val_score
    nbmodel = MultinomialNB(alpha=0.1)
    nbmodel = nbmodel.fit(X_train_dtm,Y_train)
    Y_pred = nbmodel.predict(X_test_dtm)
    cvs = cross_val_score(estimator=nbmodel, X=X_train_dtm, y=Y_train, cv=10)

    # Loop kemudian cetak hasil
    print("Cross Val Score:")
    for n, score in enumerate(cvs, start=1):
        print(f"Folds ke-{n}: {score*100:.2f}")

    cvs_list = cvs.tolist()

    # +1 pada nilai n_max dan n_min karena index dimulai dari 0
    max_cvs = max(cvs_list)
    n_max = (cvs_list.index(max_cvs)) + 1

    min_cvs = min(cvs_list)
    n_min = (cvs_list.index(min_cvs)) + 1

    print(f"Cross val score terbesar terletak pada fold ke-{n_max} dengan nilai {max_cvs*100:.2f}")
    print(f"Cross val score terkecil terletak pada fold ke-{n_min} dengan nilai {min_cvs*100:.2f}")

    # plt.rcParams["figure.figsize"] = [9,6]
    # plt.rcParams["figure.autolayout"] = True

    # Tentukan x dan y pada plot
    x = [1,2,3,4,5,6,7,8,9,10]
    y = cvs*100

    percentage = cvs*100
    ax = sns.barplot(x=x, y=y)
    patches = ax.patches

    # Tambah keterangan di atas barplot
    # for i in range(len(patches)):
    #     x = patches[i].get_x() + patches[i].get_width()/2
    #     y = patches[i].get_height()+.5
    #     ax.annotate('{:.2f}'.format(percentage[i]), (x, y), ha='center')

    # Plot barplot
    # plt.title('Cross Validasi K-Fold pada Klasifikasi Naive Bayes\n')
    # plt.xlabel('Urutan K-Fold')
    # plt.ylabel('Akurasi')
    # plt.show()

    refetch = pd.read_csv('static/datasets_anies.csv', delimiter=';')
    refetch['svm'] = data['label']
    refetch.to_csv("static/datasets_anies.csv", sep=';', index=False)

    return n_neutral, n_positif, n_negatif, data_neutral, data_positif, data_negatif, cvs, report

def DataGanjarPranowo():
    # -*- coding: utf-8 -*-
    """Sentiment Analysis Ganjar Pranowo_TF-IDF, NB, SVM.ipynb

    Automatically generated by Colaboratory.

    Original file is located at
        https://colab.research.google.com/drive/1PDh5VsDbGuPjtH59aUZe-5EWKWp7Wybb

    # About

    Sentiment analysis pada dataset twitter Ganjar Pranowo menggunakan TF-IDF, Naive Bayes, dan Support Vector Machine
    """

    # from google.colab import drive
    # drive.mount('/content/drive')

    import numpy as np
    import pandas as pd

    """# Dataset"""

    # dataset_gp = 'data/Ganjar_Labeling_3000.csv'
    dataset_gp = 'static/datasets_ganjar.csv'

    data_ganjar = pd.read_csv(dataset_gp, delimiter=';')
    data_ganjar

    data_ganjar.isnull().sum()

    df_ganjar = data_ganjar.copy()

    """mapping label yang berbentuk angka menjadi bentuk klasifikasi (positif, negatif, neutral)"""

    df_ganjar["observasi"] = df_ganjar["label"].map({0: "negatif", 1: "positif", 2: "neutral"})
    df_ganjar.head()

    # Cek sample dari data
    sample_ganjar = df_ganjar.groupby('label').apply(lambda x: x.sample(5))
    sample_ganjar

    df_ganjar["observasi"].value_counts()

    """Hitung proporsi distribusi untuk menentukan metode OVR atau OVO dalam model Naive Bayes"""

    # Tentukan distribusi masing-masing hasil labeling
    class_distribution = pd.DataFrame({'sentimen': ['positif', 'negatif', 'neutral'],
                                    'value_counts': [1398, 1119, 483]})

    # Hitung proporsi distribusi
    class_distribution['proporsi'] = (class_distribution['value_counts'] / class_distribution['value_counts'].sum()) * 100
    print(class_distribution)

    """# TF IDF dan Support Vector Machine"""

    def feature_extraction(data, method="tfidf"):
        from sklearn.feature_extraction.text import TfidfVectorizer

        feature_extraction = TfidfVectorizer(sublinear_tf=True)
        features = feature_extraction.fit_transform(data)

        return features

    def svm_classifier(features, label, classifier="svm"):
        from sklearn.metrics import roc_auc_score
        from sklearn.svm import SVC
        from sklearn.preprocessing import label_binarize

        model = SVC(probability=True)
        model.fit(features, label)
        probability_to_be_positive = model.predict_proba(features)
        predict_output = model.predict(features)

        # Konversi label multi-class ke format one-hot-encoding
        binarized_label = label_binarize(label, classes=np.unique(label))

        # Gunakan  multi_class='ovo' karena proporsi distribusi tidak seimbang
        print("auc (train data):" , roc_auc_score(
            binarized_label,
            probability_to_be_positive,
            multi_class='ovo'))

        print("top 10 scores:")
        for i in range(10):
            if predict_output[i] == 0:
                label = 'negatif'
            elif predict_output[i] == 1:
                label = 'positif'
            elif predict_output[i] == 2:
                label = 'neutral'
            print(f"{probability_to_be_positive[i]} = {predict_output[i]} ({label})")

        return model

    data = np.array(df_ganjar["tweet"])
    label = np.array(df_ganjar["label"])

    """Lakukan pembobotan untuk masing-masing kata pada tweet."""

    features = feature_extraction(data, method="tfidf")
    print(features)

    """Lakukan training menggunakan model Naive Bayes."""

    svm_train = svm_classifier(features, label, "naive_bayes")
    print(svm_train)

    import pandas as pd
    import numpy as np
    import pickle

    from sklearn.svm import SVC
    from sklearn.metrics import classification_report
    from sklearn.model_selection import train_test_split
    from sklearn.feature_extraction.text import CountVectorizer
    from sklearn.feature_extraction.text import TfidfTransformer
    from sklearn.pipeline import Pipeline

    """Lakukan pemrosesan TF-IDF dengan data tweet sebagai variabel X dan data label sebagai variabel y."""

    X = df_ganjar['tweet']
    y = df_ganjar['label']

    bow_transformer = CountVectorizer().fit(df_ganjar['tweet'])
    messages_bow = bow_transformer.transform(df_ganjar['tweet'])
    tfidf_transformer = TfidfTransformer().fit(messages_bow)
    messages_tfidf = tfidf_transformer.transform(messages_bow)

    """Lakukan kembali proses train dan split dengan rasio yang sama yaitu data train sebesar 80% dan data test sebesar 20%."""

    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)

    pipeline = Pipeline([
        ('bow',CountVectorizer()),
        ('tfidf',TfidfTransformer()),
        ('classifier',SVC())
        ])

    X_train = np.asarray(X)
    pipeline = pipeline.fit(X_train, np.asarray(y))

    """Buat data pickle setelah proses training selesai dilakukan. Pickle akan digunakan untuk menjalankan proses deep learning."""

    pickle_name = 'df_ganjar_svm.pickle'
    pickle_path = 'data/'

    with open(pickle_path+pickle_name, 'wb') as f:
        pickle.dump(pipeline, f)

    """Ganti parameter open untuk membaca atau menggunakan pickle."""

    with open(pickle_path+pickle_name, 'rb') as f:
        pickle_ganjar_svm = pickle.load(f)
        print(pickle_ganjar_svm)

    """Lakukan prediksi dengan dataset lain"""

    path_test = 'data/Ganjar_Testing_Model.csv'

    df_test = pd.read_csv(path_test, delimiter=',')
    df_test.head()

    """Berdasarkan dataset, data yang diperlukan pada feature enineering ini hanyalah data terakhir yaitu setelah pemrosesan pada tahap pre-processing selesai dilakukan."""

    df_test = df_test[["stemming"]].rename(columns={"stemming": "tweet"})
    df_test = df_test["tweet"]
    df_test = df_ganjar["tweet"]
    df_test

    null_values = df_test.isnull().sum()
    print(f"Total null values: {null_values}")

    duplikat = df_test.duplicated(keep=False).sum()
    print("Total duplikat:", duplikat)

    # Drop duplikat jika ada
    # df_test = df_test.drop_duplicates()

    duplikat = df_test.duplicated(keep=False).sum()
    print("Total duplikat:", duplikat)

    """Lakukan prediksi menggunakan pickle yang telah dibuat sebelumnya."""

    prediction = pickle_ganjar_svm.predict(np.asarray(df_test))
    prediction

    """Simpan hasil prediksi ke dalam dataset dengan cara melakukan looping sesuai hasil prediksi. Parameter yang digunakan masih sama seperti sebelumnya, yaitu:

    - Prediksi 0: Negatif
    - Prediksi 1: Positif
    - Prediksi 2: Neutral
    """

    result = []

    for i in range(len(prediction)):
        if(prediction[i] == 0):
            sentiment = 'negatif'
        elif(prediction[i] == 1):
            sentiment = 'positif'
        elif(prediction[i] == 2):
            sentiment = 'neutral'

        result.append({'tweet':df_test.iloc[i],'label':prediction[i],'prediksi':sentiment})

    data = pd.DataFrame(result)
    data

    sample_prediksi = data.groupby('prediksi').apply(lambda x: x.sample(5))
    sample_prediksi

    """Lakukan pengecekan tiap data pada masing-masing prediksi, kemudian buat wordcloudnya"""

    import nltk
    nltk.download('punkt')

    """Lakukan pengecekan pada data positif."""

    positif = data.loc[data['prediksi']=='positif', 'tweet']
    positif

    """Lakukan looping untuk melihat setiap tweet dengan sentimen positif"""

    for pos in positif[:10]:
        print(pos)

    from wordcloud import WordCloud
    import matplotlib.pyplot as plt

    # Tokenize ulang kolom "NoStopwordsTweets" agar kalimat dikonversi menjadi kata-kata.
    re_tokenize = [nltk.word_tokenize(tweet) for tweet in positif]

    # Flatten: Konversi list kata-kata ke dalam satu list
    list_kata = nltk.FreqDist([kata for tweet in re_tokenize for kata in tweet])

    # Wordcloud
    wordcloud = WordCloud(width=800, height=400, colormap='Greens').generate_from_frequencies(list_kata)
    # plt.figure(figsize=(10, 6))
    # plt.imshow(wordcloud, interpolation='bilinear')
    # plt.axis('off')
    # plt.show()

    top_10_positif = list_kata.most_common(10)

    data_positif = {
        'Kata Positif': [kata for kata, total in top_10_positif],
        'Total': [total for kata, total in top_10_positif]
    }

    tabel_positif = pd.DataFrame(data_positif)
    tabel_positif

    """Lakukan pengecekan pada data negatif."""

    negatif = data.loc[data['prediksi']=='negatif', 'tweet']
    negatif

    """Lakukan looping untuk melihat setiap tweet dengan sentimen negatif"""

    for neg in negatif[:10]:
        print(neg)

    from wordcloud import WordCloud
    import matplotlib.pyplot as plt

    # Tokenize ulang kolom "NoStopwordsTweets" agar kalimat dikonversi menjadi kata-kata.
    re_tokenize = [nltk.word_tokenize(tweet) for tweet in negatif]

    # Flatten: Konversi list kata-kata ke dalam satu list
    list_kata = nltk.FreqDist([kata for tweet in re_tokenize for kata in tweet])

    # Wordcloud
    wordcloud = WordCloud(width=800, height=400, colormap='hot').generate_from_frequencies(list_kata)
    # plt.figure(figsize=(10, 6))
    # plt.imshow(wordcloud, interpolation='bilinear')
    # plt.axis('off')
    # plt.show()

    top_10_negatif = list_kata.most_common(10)

    data_negatif = {
        'Kata Negatif': [kata for kata, total in top_10_negatif],
        'Total': [total for kata, total in top_10_negatif]
    }

    tabel_negatif = pd.DataFrame(data_negatif)
    tabel_negatif

    """Lakukan pengecekan pada data neutral."""

    neutral = data.loc[data['prediksi']=='neutral', 'tweet']
    neutral

    for net in neutral[:10]:
        print(net)

    from wordcloud import WordCloud
    import matplotlib.pyplot as plt

    # Tokenize ulang kolom "NoStopwordsTweets" agar kalimat dikonversi menjadi kata-kata.
    re_tokenize = [nltk.word_tokenize(tweet) for tweet in neutral]

    # Flatten: Konversi list kata-kata ke dalam satu list
    list_kata = nltk.FreqDist([kata for tweet in re_tokenize for kata in tweet])

    # Wordcloud
    wordcloud = WordCloud(width=800, height=400, colormap='Blues_r').generate_from_frequencies(list_kata)
    # plt.figure(figsize=(10, 6))
    # plt.imshow(wordcloud, interpolation='bilinear')
    # plt.axis('off')
    # plt.show()

    top_10_neutral = list_kata.most_common(10)

    data_neutral = {
        'Kata Neutral': [kata for kata, total in top_10_neutral],
        'Total': [total for kata, total in top_10_neutral]
    }

    tabel_neutral = pd.DataFrame(data_neutral)
    tabel_neutral

    """Lanjutkan dengan melihat banyaknya sentiment positif, negatif, dan netral dari hasil prediksi."""

    data.groupby(by='prediksi').agg('count')

    data.groupby(by='prediksi')['tweet'].agg('count')

    """Tampilkan dalam bentuk grafik untuk memudahkan dalam melihat klasifikasi sentimen."""

    list_prediksi = data.groupby(by='prediksi').agg('count').values.tolist()
    print(list_prediksi)

    n_negatif = list_prediksi[0][0]
    n_neutral = list_prediksi[1][0]
    n_positif = list_prediksi[2][0]

    import matplotlib.pyplot as plt
    import seaborn as sns

    # plt.rcParams["figure.figsize"] = [8,5]
    # plt.rcParams["figure.autolayout"] = True

    x = ['Positif', 'Negatif', 'Neutral']
    y = [n_positif, n_negatif, n_neutral]
    percentage = [n_positif, n_negatif, n_neutral]

    # ax = sns.barplot(x=x, y=y)
    # patches = ax.patches
    # for i in range(len(patches)):
    #     x = patches[i].get_x() + patches[i].get_width()/2
    #     y = patches[i].get_height()+50
    #     ax.annotate('{:}'.format(percentage[i]), (x, y), ha='center')

    # plt.title('Klasifikasi Sentiment \n')
    # plt.xlabel('Jenis Klasifikasi')
    # plt.ylabel('Jumlah')
    # plt.show()

    """Berdasarkan grafik, didapatkan data sebagai berikut.

    1. Sentiment positif merupakan sentiment terbanyak dengan total 5420 sentiment.
    2. Sentiment negatif merupakan sentiment dengan total 4877 sentiment.
    3. Sentiment neutral merupakan sentiment tersedikit dengan total 1110 sentiment.

    ## Evaluasi

    Cek confussion matrix, classification report, dan cross val score.
    """

    data2 = data.copy()
    data2.head(5)

    """Tentukan variabel X dan y terlebih dahulu agar dapat digunakan pada pemodelan."""

    # Menentukan x berdasarkan label
    data_tweet = pd.DataFrame(data2,columns=['tweet'])
    X = data_tweet['tweet']
    X

    # Menentukan y berdasarkan label
    data_label = pd.DataFrame(data2,columns=['label'])
    y = data_label['label']
    y

    """Lakukan pembagian antara data train dan data test dengan rasio 8:2 atau data train sebesar 80% dan data test sebesar 20%."""

    from sklearn.model_selection import train_test_split

    # Split dataset untuk train dan test
    X_train,X_test,Y_train,Y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    """Lakukan vektorisasi untuk TF-IDF."""

    from sklearn.feature_extraction.text import TfidfVectorizer

    # TF-IDF train-test
    vect = TfidfVectorizer(analyzer="word", min_df=0.0004, max_df=0.115, ngram_range=(1,3))
    vect.fit(X_train)
    X_train_dtm = vect.transform(X_train)
    X_test_dtm = vect.transform(X_test)

    """Lakukan perhitungan skor akurasi berdasarkan MultinomialNB."""

    from sklearn.naive_bayes import MultinomialNB
    from sklearn.metrics import accuracy_score

    # Melakukan perhitungan skor akurasi berdasarkan MultinomialNB
    svmmodel = SVC(gamma=0.1)
    svmmodel = svmmodel.fit(X_train_dtm,Y_train)
    Y_pred = svmmodel.predict(X_test_dtm)
    print(f"Akurasi: {accuracy_score(Y_test,Y_pred)*100:.2f}")

    """Tentukan hasil dari confusion matrix"""

    from sklearn.metrics import confusion_matrix

    # Menentukan confusion matrix
    cm = confusion_matrix(Y_test, Y_pred)
    print(f"Confusion Matrix:\n\n{cm}")

    sentimen = ['Negatif', 'Positif', 'Neutral']
    df_cm = pd.DataFrame(cm, index=sentimen, columns=sentimen)
    df_cm

    """Tentukan hasil laporan klasifikasi."""

    from sklearn.metrics import classification_report

    # Menentukan laporan hasil klasifikasi
    report = classification_report(Y_test, Y_pred)
    print(f"Hasil Laporan Klasifikasi:\n\n{report}")

    """Tentukan nilai cross_val_score."""

    from sklearn.model_selection import cross_val_score

    # Menentukan nilai cross_val_score
    nbmodel = MultinomialNB(alpha=0.1)
    nbmodel = nbmodel.fit(X_train_dtm,Y_train)
    Y_pred = nbmodel.predict(X_test_dtm)
    cvs = cross_val_score(estimator=nbmodel, X=X_train_dtm, y=Y_train, cv=10)

    # Loop kemudian cetak hasil
    print("Cross Val Score:")
    for n, score in enumerate(cvs, start=1):
        print(f"Folds ke-{n}: {score*100:.2f}")

    cvs_list = cvs.tolist()

    # +1 pada nilai n_max dan n_min karena index dimulai dari 0
    max_cvs = max(cvs_list)
    n_max = (cvs_list.index(max_cvs)) + 1

    min_cvs = min(cvs_list)
    n_min = (cvs_list.index(min_cvs)) + 1

    print(f"Cross val score terbesar terletak pada fold ke-{n_max} dengan nilai {max_cvs*100:.2f}")
    print(f"Cross val score terkecil terletak pada fold ke-{n_min} dengan nilai {min_cvs*100:.2f}")

    plt.rcParams["figure.figsize"] = [9,6]
    plt.rcParams["figure.autolayout"] = True

    # Tentukan x dan y pada plot
    x = [1,2,3,4,5,6,7,8,9,10]
    y = cvs*100

    percentage = cvs*100
    ax = sns.barplot(x=x, y=y)
    patches = ax.patches

    # Tambah keterangan di atas barplot
    # for i in range(len(patches)):
    #     x = patches[i].get_x() + patches[i].get_width()/2
    #     y = patches[i].get_height()+.5
    #     ax.annotate('{:.2f}'.format(percentage[i]), (x, y), ha='center')

    # # Plot barplot
    # plt.title('Cross Validasi K-Fold pada Klasifikasi Naive Bayes\n')
    # plt.xlabel('Urutan K-Fold')
    # plt.ylabel('Akurasi')
    # plt.show()

    refetch = pd.read_csv('static/datasets_ganjar.csv', delimiter=';')
    refetch['svm'] = data['label']
    refetch.to_csv("static/datasets_ganjar.csv", sep=';', index=False)

    return n_neutral, n_positif, n_negatif, data_neutral, data_positif, data_negatif, cvs, report

def DataAniesBaswedanNB():
    # -*- coding: utf-8 -*-
    """Sentiment Analysis Anies Baswedan_TF-IDF, NB, SVM.ipynb

    Automatically generated by Colaboratory.

    Original file is located at
        https://colab.research.google.com/drive/1cQsWcB2sX-MCyFSriMjqvEdcz0SIck_0

    # About

    Sentiment analysis pada dataset twitter Anies Baswedan menggunakan TF-IDF, Naive Bayes, dan Support Vector Machine
    """

    # from google.colab import drive
    # drive.mount('/content/drive')

    import numpy as np
    import pandas as pd

    """# Dataset"""

    # dataset_ab = 'data/Anies_Labeling_3000.csv'
    dataset_ab = 'static/datasets_anies.csv'

    data_anies = pd.read_csv(dataset_ab, delimiter=';')
    data_anies

    data_anies.isnull().sum()

    df_anies = data_anies.copy()

    """mapping label yang berbentuk angka menjadi bentuk klasifikasi (positif, negatif, neutral)"""

    df_anies["observasi"] = df_anies["label"].map({0: "negatif", 1: "positif", 2: "neutral"})
    df_anies.head()

    # Cek sample dari data
    sample_anies = df_anies.groupby('label').apply(lambda x: x.sample(5))
    sample_anies

    df_anies["observasi"].value_counts()

    """Hitung proporsi distribusi untuk menentukan metode OVR atau OVO dalam model Naive Bayes"""

    # Tentukan distribusi masing-masing hasil labeling
    class_distribution = pd.DataFrame({'sentimen': ['positif', 'negatif', 'neutral'],
                                    'value_counts': [1398, 1119, 483]})

    # Hitung proporsi distribusi
    class_distribution['proporsi'] = (class_distribution['value_counts'] / class_distribution['value_counts'].sum()) * 100
    print(class_distribution)

    """# TF IDF dan Naive Bayes"""

    def feature_extraction(data, method="tfidf"):
        from sklearn.feature_extraction.text import TfidfVectorizer

        feature_extraction = TfidfVectorizer(sublinear_tf=True)
        features = feature_extraction.fit_transform(data)

        return features

    def nb_classifier(features, label, classifier="naive_bayes"):
        from sklearn.metrics import roc_auc_score
        from sklearn.naive_bayes import MultinomialNB
        from sklearn.preprocessing import label_binarize

        model = MultinomialNB()
        model.fit(features, label)
        probability_to_be_positive = model.predict_proba(features)
        predict_output = model.predict(features)

        # Konversi label multi-class ke format one-hot-encoding
        binarized_label = label_binarize(label, classes=np.unique(label))

        # Gunakan  multi_class='ovo' karena proporsi distribusi tidak seimbang
        print("auc (train data):" , roc_auc_score(
            binarized_label,
            probability_to_be_positive,
            multi_class='ovo'))

        print("top 10 scores:")
        for i in range(10):
            if predict_output[i] == 0:
                label = 'negatif'
            elif predict_output[i] == 1:
                label = 'positif'
            elif predict_output[i] == 2:
                label = 'neutral'
            print(f"{probability_to_be_positive[i]} = {predict_output[i]} ({label})")

        return model

    data = np.array(df_anies["tweet"])
    label = np.array(df_anies["label"])

    """Lakukan pembobotan untuk masing-masing kata pada tweet."""

    features = feature_extraction(data, method="tfidf")
    print(features)

    """Lakukan training menggunakan model Naive Bayes."""

    nb_train = nb_classifier(features, label, "naive_bayes")
    print(nb_train)

    import pandas as pd
    import numpy as np
    import pickle

    from sklearn.naive_bayes import MultinomialNB
    from sklearn.metrics import classification_report
    from sklearn.model_selection import train_test_split
    from sklearn.feature_extraction.text import CountVectorizer
    from sklearn.feature_extraction.text import TfidfTransformer
    from sklearn.pipeline import Pipeline

    """Lakukan pemrosesan TF-IDF dengan data tweet sebagai variabel X dan data label sebagai variabel y."""

    X = df_anies['tweet']
    y = df_anies['label']

    bow_transformer = CountVectorizer().fit(df_anies['tweet'])
    messages_bow = bow_transformer.transform(df_anies['tweet'])
    tfidf_transformer = TfidfTransformer().fit(messages_bow)
    messages_tfidf = tfidf_transformer.transform(messages_bow)

    """Lakukan kembali proses train dan split dengan rasio yang sama yaitu data train sebesar 80% dan data test sebesar 20%."""

    X_train,X_test,y_train,y_test = train_test_split(df_anies['tweet'],df_anies['label'],test_size=0.2,random_state=42)

    pipeline = Pipeline([
        ('bow',CountVectorizer()),
        ('tfidf',TfidfTransformer()),
        ('classifier',MultinomialNB())
        ])

    X_train = np.asarray(X)
    pipeline = pipeline.fit(X_train, np.asarray(y))

    """Buat data pickle setelah proses training selesai dilakukan. Pickle akan digunakan untuk menjalankan proses deep learning."""

    pickle_name = 'df_anies_nb.pickle'
    pickle_path = 'data/'

    with open(pickle_path+pickle_name, 'wb') as f:
        pickle.dump(pipeline, f)

    """Ganti parameter open untuk membaca atau menggunakan pickle."""

    with open(pickle_path+pickle_name, 'rb') as f:
        pickle_anies = pickle.load(f)
        print(pickle_anies)

    """Lakukan prediksi dengan dataset lain"""

    path_test = 'data/Anies_Testing_Model.csv'

    df_test = pd.read_csv(path_test, delimiter=',')
    df_test.head()

    """Berdasarkan dataset, data yang diperlukan pada feature enineering ini hanyalah data terakhir yaitu setelah pemrosesan pada tahap pre-processing selesai dilakukan."""

    df_test = df_test[["stemming"]].rename(columns={"stemming": "tweet"})
    df_test = df_test["tweet"]
    df_test = df_anies["tweet"]
    df_test

    null_values = df_test.isnull().sum()
    print(f"Total null values: {null_values}")

    duplikat = df_test.duplicated(keep=False).sum()
    print("Total duplikat:", duplikat)

    # Drop duplikat jika ada
    # df_test = df_test.drop_duplicates()

    duplikat = df_test.duplicated(keep=False).sum()
    print("Total duplikat:", duplikat)

    """Lakukan prediksi menggunakan pickle yang telah dibuat sebelumnya."""

    prediction = pickle_anies.predict(np.asarray(df_test))
    prediction

    """Simpan hasil prediksi ke dalam dataset dengan cara melakukan looping sesuai hasil prediksi. Parameter yang digunakan masih sama seperti sebelumnya, yaitu:

    - Prediksi 0: Negatif
    - Prediksi 1: Positif
    - Prediksi 2: Neutral
    """

    result = []

    for i in range(len(prediction)):
        if(prediction[i] == 0):
            sentiment = 'negatif'
        elif(prediction[i] == 1):
            sentiment = 'positif'
        elif(prediction[i] == 2):
            sentiment = 'neutral'

        result.append({'tweet':df_test.iloc[i],'label':prediction[i],'prediksi':sentiment})

    data = pd.DataFrame(result)
    data

    sample_prediksi = data.groupby('prediksi').apply(lambda x: x.sample(5))
    sample_prediksi

    """Lakukan pengecekan tiap data pada masing-masing prediksi, kemudian buat wordcloudnya"""

    import nltk
    nltk.download('punkt')

    """Lakukan pengecekan pada data positif."""

    positif = data.loc[data['prediksi']=='positif', 'tweet']
    positif

    """Lakukan looping untuk melihat setiap tweet dengan sentimen positif"""

    for pos in positif[:10]:
        print(pos)

    from wordcloud import WordCloud
    import matplotlib.pyplot as plt

    # Tokenize ulang kolom "NoStopwordsTweets" agar kalimat dikonversi menjadi kata-kata.
    re_tokenize = [nltk.word_tokenize(tweet) for tweet in positif]

    # Flatten: Konversi list kata-kata ke dalam satu list
    list_kata = nltk.FreqDist([kata for tweet in re_tokenize for kata in tweet])

    # Wordcloud
    wordcloud = WordCloud(width=800, height=400, colormap='Greens').generate_from_frequencies(list_kata)
    # plt.figure(figsize=(10, 6))
    # plt.imshow(wordcloud, interpolation='bilinear')
    # plt.axis('off')
    # plt.show()

    top_10_positif = list_kata.most_common(10)

    data_positif = {
        'Kata Positif': [kata for kata, total in top_10_positif],
        'Total': [total for kata, total in top_10_positif]
    }

    tabel_positif = pd.DataFrame(data_positif)
    tabel_positif

    """Lakukan pengecekan pada data negatif."""

    negatif = data.loc[data['prediksi']=='negatif', 'tweet']
    negatif

    """Lakukan looping untuk melihat setiap tweet dengan sentimen negatif"""

    for neg in negatif[:10]:
        print(neg)

    from wordcloud import WordCloud
    import matplotlib.pyplot as plt

    # Tokenize ulang kolom "NoStopwordsTweets" agar kalimat dikonversi menjadi kata-kata.
    re_tokenize = [nltk.word_tokenize(tweet) for tweet in negatif]

    # Flatten: Konversi list kata-kata ke dalam satu list
    list_kata = nltk.FreqDist([kata for tweet in re_tokenize for kata in tweet])

    # Wordcloud
    wordcloud = WordCloud(width=800, height=400, colormap='hot').generate_from_frequencies(list_kata)
    # plt.figure(figsize=(10, 6))
    # plt.imshow(wordcloud, interpolation='bilinear')
    # plt.axis('off')
    # plt.show()

    top_10_negatif = list_kata.most_common(10)

    data_negatif = {
        'Kata Negatif': [kata for kata, total in top_10_negatif],
        'Total': [total for kata, total in top_10_negatif]
    }

    tabel_negatif = pd.DataFrame(data_negatif)
    tabel_negatif

    """Lakukan pengecekan pada data neutral."""

    neutral = data.loc[data['prediksi']=='neutral', 'tweet']
    neutral

    for net in neutral[:10]:
        print(net)

    from wordcloud import WordCloud
    import matplotlib.pyplot as plt

    # Tokenize ulang kolom "NoStopwordsTweets" agar kalimat dikonversi menjadi kata-kata.
    re_tokenize = [nltk.word_tokenize(tweet) for tweet in neutral]

    # Flatten: Konversi list kata-kata ke dalam satu list
    list_kata = nltk.FreqDist([kata for tweet in re_tokenize for kata in tweet])

    # Wordcloud
    wordcloud = WordCloud(width=800, height=400, colormap='Blues_r').generate_from_frequencies(list_kata)
    # plt.figure(figsize=(10, 6))
    # plt.imshow(wordcloud, interpolation='bilinear')
    # plt.axis('off')
    # plt.show()

    top_10_neutral = list_kata.most_common(10)

    data_neutral = {
        'Kata Neutral': [kata for kata, total in top_10_neutral],
        'Total': [total for kata, total in top_10_neutral]
    }

    tabel_neutral = pd.DataFrame(data_neutral)
    tabel_neutral

    """Lanjutkan dengan melihat banyaknya sentiment positif, negatif, dan netral dari hasil prediksi."""

    data.groupby(by='prediksi').agg('count')

    data.groupby(by='prediksi')['tweet'].agg('count')

    """Tampilkan dalam bentuk grafik untuk memudahkan dalam melihat klasifikasi sentimen."""

    list_prediksi = data.groupby(by='prediksi').agg('count').values.tolist()
    print(list_prediksi)

    n_negatif = list_prediksi[0][0]
    n_neutral = list_prediksi[1][0]
    n_positif = list_prediksi[2][0]

    import matplotlib.pyplot as plt
    import seaborn as sns

    # plt.rcParams["figure.figsize"] = [8,5]
    # plt.rcParams["figure.autolayout"] = True

    x = ['Positif', 'Negatif', 'Neutral']
    y = [n_positif, n_negatif, n_neutral]
    percentage = [n_positif, n_negatif, n_neutral]

    # ax = sns.barplot(x=x, y=y)
    # patches = ax.patches
    # for i in range(len(patches)):
    #     x = patches[i].get_x() + patches[i].get_width()/2
    #     y = patches[i].get_height()+50
    #     ax.annotate('{:}'.format(percentage[i]), (x, y), ha='center')

    # plt.title('Klasifikasi Sentiment \n')
    # plt.xlabel('Jenis Klasifikasi')
    # plt.ylabel('Jumlah')
    # plt.show()

    """Berdasarkan grafik, didapatkan data sebagai berikut.

    1. Sentiment positif merupakan sentiment terbanyak dengan total 6263 sentiment.
    2. Sentiment negatif merupakan sentiment dengan total 4415 sentiment.
    3. Sentiment neutral merupakan sentiment tersedikit dengan total 729 sentiment.

    ## Evaluasi

    Cek confussion matrix, classification report, dan cross val score.
    """

    data2 = data.copy()
    data2.head(5)

    """Tentukan variabel X dan y terlebih dahulu agar dapat digunakan pada pemodelan."""

    # Menentukan x berdasarkan label
    data_tweet = pd.DataFrame(data2,columns=['tweet'])
    X = data_tweet['tweet']
    X

    # Menentukan y berdasarkan label
    data_label = pd.DataFrame(data2,columns=['label'])
    y = data_label['label']
    y

    """Lakukan pembagian antara data train dan data test dengan rasio 8:2 atau data train sebesar 80% dan data test sebesar 20%."""

    from sklearn.model_selection import train_test_split

    # Split dataset untuk train dan test
    X_train,X_test,Y_train,Y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    """Lakukan vektorisasi untuk TF-IDF."""

    from sklearn.feature_extraction.text import TfidfVectorizer

    # TF-IDF train-test
    vect = TfidfVectorizer(analyzer="word", min_df=0.0004, max_df=0.115, ngram_range=(1,3))
    vect.fit(X_train)
    X_train_dtm = vect.transform(X_train)
    X_test_dtm = vect.transform(X_test)

    """Lakukan perhitungan skor akurasi berdasarkan MultinomialNB."""

    from sklearn.naive_bayes import MultinomialNB
    from sklearn.metrics import accuracy_score

    # Melakukan perhitungan skor akurasi berdasarkan MultinomialNB
    nbmodel = MultinomialNB(alpha=0.1)
    nbmodel = nbmodel.fit(X_train_dtm,Y_train)
    Y_pred = nbmodel.predict(X_test_dtm)
    print(f"Akurasi: {accuracy_score(Y_test,Y_pred)*100:.2f}")

    """Tentukan hasil dari confusion matrix"""

    from sklearn.metrics import confusion_matrix

    # Menentukan confusion matrix
    cm = confusion_matrix(Y_test, Y_pred)
    print(f"Confusion Matrix:\n\n{cm}")

    sentimen = ['Negatif', 'Positif', 'Neutral']
    df_cm = pd.DataFrame(cm, index=sentimen, columns=sentimen)
    df_cm

    """Tentukan hasil laporan klasifikasi."""

    from sklearn.metrics import classification_report

    # Menentukan laporan hasil klasifikasi
    report = classification_report(Y_test, Y_pred)
    print(f"Hasil Laporan Klasifikasi:\n\n{report}")

    """Tentukan nilai cross_val_score."""

    from sklearn.model_selection import cross_val_score

    # Menentukan nilai cross_val_score
    nbmodel = MultinomialNB(alpha=0.1)
    nbmodel = nbmodel.fit(X_train_dtm,Y_train)
    Y_pred = nbmodel.predict(X_test_dtm)
    cvs = cross_val_score(estimator=nbmodel, X=X_train_dtm, y=Y_train, cv=10)

    # Loop kemudian cetak hasil
    print("Cross Val Score:")
    for n, score in enumerate(cvs, start=1):
        print(f"Folds ke-{n}: {score*100:.2f}")

    cvs_list = cvs.tolist()

    # +1 pada nilai n_max dan n_min karena index dimulai dari 0
    max_cvs = max(cvs_list)
    n_max = (cvs_list.index(max_cvs)) + 1

    min_cvs = min(cvs_list)
    n_min = (cvs_list.index(min_cvs)) + 1

    print(f"Cross val score terbesar terletak pada fold ke-{n_max} dengan nilai {max_cvs*100:.2f}")
    print(f"Cross val score terkecil terletak pada fold ke-{n_min} dengan nilai {min_cvs*100:.2f}")

    plt.rcParams["figure.figsize"] = [9,6]
    plt.rcParams["figure.autolayout"] = True

    # Tentukan x dan y pada plot
    x = [1,2,3,4,5,6,7,8,9,10]
    y = cvs*100

    percentage = cvs*100
    ax = sns.barplot(x=x, y=y)
    patches = ax.patches

    refetch = pd.read_csv('static/datasets_anies.csv', delimiter=';')
    refetch['nb'] = data['label']
    refetch.to_csv("static/datasets_anies.csv", sep=';', index=False)

    return n_neutral, n_positif, n_negatif, data_neutral, data_positif, data_negatif, cvs, report

def DataGanjarPranowoNB():
    # -*- coding: utf-8 -*-
    """Sentiment Analysis Ganjar Pranowo_TF-IDF, NB, SVM.ipynb

    Automatically generated by Colaboratory.

    Original file is located at
        https://colab.research.google.com/drive/1PDh5VsDbGuPjtH59aUZe-5EWKWp7Wybb

    # About

    Sentiment analysis pada dataset twitter Ganjar Pranowo menggunakan TF-IDF, Naive Bayes, dan Support Vector Machine
    """

    # from google.colab import drive
    # drive.mount('/content/drive')

    import numpy as np
    import pandas as pd

    """# Dataset"""

    # dataset_gp = 'data/Ganjar_Labeling_3000.csv'
    dataset_gp = 'static/datasets_ganjar.csv'

    data_ganjar = pd.read_csv(dataset_gp, delimiter=';')
    data_ganjar

    data_ganjar.isnull().sum()

    df_ganjar = data_ganjar.copy()

    """mapping label yang berbentuk angka menjadi bentuk klasifikasi (positif, negatif, neutral)"""

    df_ganjar["observasi"] = df_ganjar["label"].map({0: "negatif", 1: "positif", 2: "neutral"})
    df_ganjar.head()

    # Cek sample dari data
    sample_ganjar = df_ganjar.groupby('label').apply(lambda x: x.sample(5))
    sample_ganjar

    df_ganjar["observasi"].value_counts()

    """Hitung proporsi distribusi untuk menentukan metode OVR atau OVO dalam model Naive Bayes"""

    # Tentukan distribusi masing-masing hasil labeling
    class_distribution = pd.DataFrame({'sentimen': ['positif', 'negatif', 'neutral'],
                                    'value_counts': [1398, 1119, 483]})

    # Hitung proporsi distribusi
    class_distribution['proporsi'] = (class_distribution['value_counts'] / class_distribution['value_counts'].sum()) * 100
    print(class_distribution)

    """# TF IDF dan Naive Bayes"""

    def feature_extraction(data, method="tfidf"):
        from sklearn.feature_extraction.text import TfidfVectorizer

        feature_extraction = TfidfVectorizer(sublinear_tf=True)
        features = feature_extraction.fit_transform(data)

        return features

    def nb_classifier(features, label, classifier="naive_bayes"):
        from sklearn.metrics import roc_auc_score
        from sklearn.naive_bayes import MultinomialNB
        from sklearn.preprocessing import label_binarize

        model = MultinomialNB()
        model.fit(features, label)
        probability_to_be_positive = model.predict_proba(features)
        predict_output = model.predict(features)

        # Konversi label multi-class ke format one-hot-encoding
        binarized_label = label_binarize(label, classes=np.unique(label))

        # Gunakan  multi_class='ovo' karena proporsi distribusi tidak seimbang
        print("auc (train data):" , roc_auc_score(
            binarized_label,
            probability_to_be_positive,
            multi_class='ovo'))

        print("top 10 scores:")
        for i in range(10):
            if predict_output[i] == 0:
                label = 'negatif'
            elif predict_output[i] == 1:
                label = 'positif'
            elif predict_output[i] == 2:
                label = 'neutral'
            print(f"{probability_to_be_positive[i]} = {predict_output[i]} ({label})")

        return model

    data = np.array(df_ganjar["tweet"])
    label = np.array(df_ganjar["label"])

    """Lakukan pembobotan untuk masing-masing kata pada tweet."""

    features = feature_extraction(data, method="tfidf")
    print(features)

    """Lakukan training menggunakan model Naive Bayes."""

    nb_train = nb_classifier(features, label, "naive_bayes")
    print(nb_train)

    import pandas as pd
    import numpy as np
    import pickle

    from sklearn.naive_bayes import MultinomialNB
    from sklearn.metrics import classification_report
    from sklearn.model_selection import train_test_split
    from sklearn.feature_extraction.text import CountVectorizer
    from sklearn.feature_extraction.text import TfidfTransformer
    from sklearn.pipeline import Pipeline

    """Lakukan pemrosesan TF-IDF dengan data tweet sebagai variabel X dan data label sebagai variabel y."""

    X = df_ganjar['tweet']
    y = df_ganjar['label']

    bow_transformer = CountVectorizer().fit(df_ganjar['tweet'])
    messages_bow = bow_transformer.transform(df_ganjar['tweet'])
    tfidf_transformer = TfidfTransformer().fit(messages_bow)
    messages_tfidf = tfidf_transformer.transform(messages_bow)

    """Lakukan kembali proses train dan split dengan rasio yang sama yaitu data train sebesar 80% dan data test sebesar 20%."""

    X_train,X_test,y_train,y_test = train_test_split(df_ganjar['tweet'],df_ganjar['label'],test_size=0.2,random_state=42)

    pipeline = Pipeline([
        ('bow',CountVectorizer()),
        ('tfidf',TfidfTransformer()),
        ('classifier',MultinomialNB())
        ])

    X_train = np.asarray(X)
    pipeline = pipeline.fit(X_train, np.asarray(y))

    """Buat data pickle setelah proses training selesai dilakukan. Pickle akan digunakan untuk menjalankan proses deep learning."""

    pickle_name = 'df_ganjar.pickle'
    pickle_path = 'data/'

    with open(pickle_path+pickle_name, 'wb') as f:
        pickle.dump(pipeline, f)

    """Ganti parameter open untuk membaca atau menggunakan pickle."""

    with open(pickle_path+pickle_name, 'rb') as f:
        pickle_ganjar = pickle.load(f)
        print(pickle_ganjar)

    """Lakukan prediksi dengan dataset lain"""

    path_test = 'data/Ganjar_Testing_Model.csv'

    df_test = pd.read_csv(path_test, delimiter=',')
    df_test.head()

    """Berdasarkan dataset, data yang diperlukan pada feature enineering ini hanyalah data terakhir yaitu setelah pemrosesan pada tahap pre-processing selesai dilakukan."""

    df_test = df_test[["stemming"]].rename(columns={"stemming": "tweet"})
    df_test = df_test["tweet"]
    df_test = df_ganjar["tweet"]
    df_test

    null_values = df_test.isnull().sum()
    print(f"Total null values: {null_values}")

    duplikat = df_test.duplicated(keep=False).sum()
    print("Total duplikat:", duplikat)

    # Drop duplikat jika ada
    # df_test = df_test.drop_duplicates()

    duplikat = df_test.duplicated(keep=False).sum()
    print("Total duplikat:", duplikat)

    """Lakukan prediksi menggunakan pickle yang telah dibuat sebelumnya."""

    prediction = pickle_ganjar.predict(np.asarray(df_test))
    prediction

    """Simpan hasil prediksi ke dalam dataset dengan cara melakukan looping sesuai hasil prediksi. Parameter yang digunakan masih sama seperti sebelumnya, yaitu:

    - Prediksi 0: Negatif
    - Prediksi 1: Positif
    - Prediksi 2: Neutral
    """

    result = []

    for i in range(len(prediction)):
        if(prediction[i] == 0):
            sentiment = 'negatif'
        elif(prediction[i] == 1):
            sentiment = 'positif'
        elif(prediction[i] == 2):
            sentiment = 'neutral'

        result.append({'tweet':df_test.iloc[i],'label':prediction[i],'prediksi':sentiment})

    data = pd.DataFrame(result)
    data

    sample_prediksi = data.groupby('prediksi').apply(lambda x: x.sample(5))
    sample_prediksi

    """Lakukan pengecekan tiap data pada masing-masing prediksi, kemudian buat wordcloudnya"""

    import nltk
    nltk.download('punkt')

    """Lakukan pengecekan pada data positif."""

    positif = data.loc[data['prediksi']=='positif', 'tweet']
    positif

    """Lakukan looping untuk melihat setiap tweet dengan sentimen positif"""

    for pos in positif[:10]:
        print(pos)

    from wordcloud import WordCloud
    import matplotlib.pyplot as plt

    # Tokenize ulang kolom "NoStopwordsTweets" agar kalimat dikonversi menjadi kata-kata.
    re_tokenize = [nltk.word_tokenize(tweet) for tweet in positif]

    # Flatten: Konversi list kata-kata ke dalam satu list
    list_kata = nltk.FreqDist([kata for tweet in re_tokenize for kata in tweet])

    # Wordcloud
    wordcloud = WordCloud(width=800, height=400, colormap='Greens').generate_from_frequencies(list_kata)
    # plt.figure(figsize=(10, 6))
    # plt.imshow(wordcloud, interpolation='bilinear')
    # plt.axis('off')
    # plt.show()

    top_10_positif = list_kata.most_common(10)

    data_positif = {
        'Kata Positif': [kata for kata, total in top_10_positif],
        'Total': [total for kata, total in top_10_positif]
    }

    tabel_positif = pd.DataFrame(data_positif)
    tabel_positif

    """Lakukan pengecekan pada data negatif."""

    negatif = data.loc[data['prediksi']=='negatif', 'tweet']
    negatif

    """Lakukan looping untuk melihat setiap tweet dengan sentimen negatif"""

    for neg in negatif[:10]:
        print(neg)

    from wordcloud import WordCloud
    import matplotlib.pyplot as plt

    # Tokenize ulang kolom "NoStopwordsTweets" agar kalimat dikonversi menjadi kata-kata.
    re_tokenize = [nltk.word_tokenize(tweet) for tweet in negatif]

    # Flatten: Konversi list kata-kata ke dalam satu list
    list_kata = nltk.FreqDist([kata for tweet in re_tokenize for kata in tweet])

    # Wordcloud
    wordcloud = WordCloud(width=800, height=400, colormap='hot').generate_from_frequencies(list_kata)
    # plt.figure(figsize=(10, 6))
    # plt.imshow(wordcloud, interpolation='bilinear')
    # plt.axis('off')
    # plt.show()

    top_10_negatif = list_kata.most_common(10)

    data_negatif = {
        'Kata Negatif': [kata for kata, total in top_10_negatif],
        'Total': [total for kata, total in top_10_negatif]
    }

    tabel_negatif = pd.DataFrame(data_negatif)
    tabel_negatif

    """Lakukan pengecekan pada data neutral."""

    neutral = data.loc[data['prediksi']=='neutral', 'tweet']
    neutral

    for net in neutral[:10]:
        print(net)

    from wordcloud import WordCloud
    import matplotlib.pyplot as plt

    # Tokenize ulang kolom "NoStopwordsTweets" agar kalimat dikonversi menjadi kata-kata.
    re_tokenize = [nltk.word_tokenize(tweet) for tweet in neutral]

    # Flatten: Konversi list kata-kata ke dalam satu list
    list_kata = nltk.FreqDist([kata for tweet in re_tokenize for kata in tweet])

    # Wordcloud
    wordcloud = WordCloud(width=800, height=400, colormap='Blues_r').generate_from_frequencies(list_kata)
    # plt.figure(figsize=(10, 6))
    # plt.imshow(wordcloud, interpolation='bilinear')
    # plt.axis('off')
    # plt.show()

    top_10_neutral = list_kata.most_common(10)

    data_neutral = {
        'Kata Neutral': [kata for kata, total in top_10_neutral],
        'Total': [total for kata, total in top_10_neutral]
    }

    tabel_neutral = pd.DataFrame(data_neutral)
    tabel_neutral

    """Lanjutkan dengan melihat banyaknya sentiment positif, negatif, dan netral dari hasil prediksi."""

    data.groupby(by='prediksi').agg('count')

    data.groupby(by='prediksi')['tweet'].agg('count')

    """Tampilkan dalam bentuk grafik untuk memudahkan dalam melihat klasifikasi sentimen."""

    list_prediksi = data.groupby(by='prediksi').agg('count').values.tolist()
    print(list_prediksi)

    n_negatif = list_prediksi[0][0]
    n_neutral = list_prediksi[1][0]
    n_positif = list_prediksi[2][0]

    import matplotlib.pyplot as plt
    import seaborn as sns

    # plt.rcParams["figure.figsize"] = [8,5]
    # plt.rcParams["figure.autolayout"] = True

    x = ['Positif', 'Negatif', 'Neutral']
    y = [n_positif, n_negatif, n_neutral]
    percentage = [n_positif, n_negatif, n_neutral]

    # ax = sns.barplot(x=x, y=y)
    # patches = ax.patches
    # for i in range(len(patches)):
    #     x = patches[i].get_x() + patches[i].get_width()/2
    #     y = patches[i].get_height()+50
    #     ax.annotate('{:}'.format(percentage[i]), (x, y), ha='center')

    # plt.title('Klasifikasi Sentiment \n')
    # plt.xlabel('Jenis Klasifikasi')
    # plt.ylabel('Jumlah')
    # plt.show()

    """Berdasarkan grafik, didapatkan data sebagai berikut.

    1. Sentiment positif merupakan sentiment terbanyak dengan total 6263 sentiment.
    2. Sentiment negatif merupakan sentiment dengan total 4415 sentiment.
    3. Sentiment neutral merupakan sentiment tersedikit dengan total 729 sentiment.

    ## Evaluasi

    Cek confussion matrix, classification report, dan cross val score.
    """

    data2 = data.copy()
    data2.head(5)

    """Tentukan variabel X dan y terlebih dahulu agar dapat digunakan pada pemodelan."""

    # Menentukan x berdasarkan label
    data_tweet = pd.DataFrame(data2,columns=['tweet'])
    X = data_tweet['tweet']
    X

    # Menentukan y berdasarkan label
    data_label = pd.DataFrame(data2,columns=['label'])
    y = data_label['label']
    y

    """Lakukan pembagian antara data train dan data test dengan rasio 8:2 atau data train sebesar 80% dan data test sebesar 20%."""

    from sklearn.model_selection import train_test_split

    # Split dataset untuk train dan test
    X_train,X_test,Y_train,Y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    """Lakukan vektorisasi untuk TF-IDF."""

    from sklearn.feature_extraction.text import TfidfVectorizer

    # TF-IDF train-test
    vect = TfidfVectorizer(analyzer="word", min_df=0.0004, max_df=0.115, ngram_range=(1,3))
    vect.fit(X_train)
    X_train_dtm = vect.transform(X_train)
    X_test_dtm = vect.transform(X_test)

    """Lakukan perhitungan skor akurasi berdasarkan MultinomialNB."""

    from sklearn.naive_bayes import MultinomialNB
    from sklearn.metrics import accuracy_score

    # Melakukan perhitungan skor akurasi berdasarkan MultinomialNB
    nbmodel = MultinomialNB(alpha=0.1)
    nbmodel = nbmodel.fit(X_train_dtm,Y_train)
    Y_pred = nbmodel.predict(X_test_dtm)
    print(f"Akurasi: {accuracy_score(Y_test,Y_pred)*100:.2f}")

    """Tentukan hasil dari confusion matrix"""

    from sklearn.metrics import confusion_matrix

    # Menentukan confusion matrix
    cm = confusion_matrix(Y_test, Y_pred)
    print(f"Confusion Matrix:\n\n{cm}")

    sentimen = ['Negatif', 'Positif', 'Neutral']
    df_cm = pd.DataFrame(cm, index=sentimen, columns=sentimen)
    df_cm

    """Tentukan hasil laporan klasifikasi."""

    from sklearn.metrics import classification_report

    # Menentukan laporan hasil klasifikasi
    report = classification_report(Y_test, Y_pred)
    print(f"Hasil Laporan Klasifikasi:\n\n{report}")

    """Tentukan nilai cross_val_score."""

    from sklearn.model_selection import cross_val_score

    # Menentukan nilai cross_val_score
    nbmodel = MultinomialNB(alpha=0.1)
    nbmodel = nbmodel.fit(X_train_dtm,Y_train)
    Y_pred = nbmodel.predict(X_test_dtm)
    cvs = cross_val_score(estimator=nbmodel, X=X_train_dtm, y=Y_train, cv=10)

    # Loop kemudian cetak hasil
    print("Cross Val Score:")
    for n, score in enumerate(cvs, start=1):
        print(f"Folds ke-{n}: {score*100:.2f}")

    cvs_list = cvs.tolist()

    # +1 pada nilai n_max dan n_min karena index dimulai dari 0
    max_cvs = max(cvs_list)
    n_max = (cvs_list.index(max_cvs)) + 1

    min_cvs = min(cvs_list)
    n_min = (cvs_list.index(min_cvs)) + 1

    print(f"Cross val score terbesar terletak pada fold ke-{n_max} dengan nilai {max_cvs*100:.2f}")
    print(f"Cross val score terkecil terletak pada fold ke-{n_min} dengan nilai {min_cvs*100:.2f}")

    plt.rcParams["figure.figsize"] = [9,6]
    plt.rcParams["figure.autolayout"] = True

    # Tentukan x dan y pada plot
    x = [1,2,3,4,5,6,7,8,9,10]
    y = cvs*100

    percentage = cvs*100
    ax = sns.barplot(x=x, y=y)
    patches = ax.patches

    # Tambah keterangan di atas barplot
    # for i in range(len(patches)):
    #     x = patches[i].get_x() + patches[i].get_width()/2
    #     y = patches[i].get_height()+.5
    #     ax.annotate('{:.2f}'.format(percentage[i]), (x, y), ha='center')

    # Plot barplot
    # plt.title('Cross Validasi K-Fold pada Klasifikasi Naive Bayes\n')
    # plt.xlabel('Urutan K-Fold')
    # plt.ylabel('Akurasi')
    # plt.show()

    refetch = pd.read_csv('static/datasets_ganjar.csv', delimiter=';')
    refetch['nb'] = data['label']
    refetch.to_csv("static/datasets_ganjar.csv", sep=';', index=False)

    return n_neutral, n_positif, n_negatif, data_neutral, data_positif, data_negatif, cvs, report

def where_json(file_name):
    return os.path.exists(file_name)

def _PreProcessing(text):
    def casefolding(tweet):#function
        tweet = tweet.lower()
        return tweet
    def cleaning(tweet):#function
        import re
        tweet = re.sub(r'https\S+|www\S+https\S+', '',tweet, flags=re.MULTILINE) #link
        tweet = re.sub(r'@\w+|#\w+', '', tweet) #mention atau username
        tweet = re.sub(r"\b[a-zA-Z]\b", '', tweet) #remove single char
        tweet = re.sub(r'[^\w\s]', '', tweet) #spasi
        tweet = re.sub(r"[0-9]+",'',tweet) #angka
        tweet = re.sub(r"[\n]", ' ' ,tweet) #enter
        tweet = re.sub(r'[\s]+', ' ',tweet) #double whitespace ''
        return tweet
    # import word_tokenize dari library nltk
    import nltk
    from nltk.tokenize import word_tokenize
    def tokenizing(tweet):
        tweet_tokens = word_tokenize(tweet)
        return tweet_tokens
    from nltk.corpus import stopwords
    def stopwordRemoval(tweet):
        tweet_tokens = word_tokenize(tweet)
        stop_words = set(stopwords.words('indonesian'))
        filtered_tweet = [w for w in tweet_tokens if not w in stop_words]
        return filtered_tweet
    from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
    fact = StemmerFactory()
    stemmer = fact.create_stemmer()
    def stemming(data):
        tweet = [stemmer.stem(word) for word in data]
        return ' '.join(tweet)

    text = casefolding(text)
    text = cleaning(text)
    # text = tokenizing(text)
    text = stopwordRemoval(text)
    text = stemming(text)

    return text

def _Prediksi(text, args):
    # -*- coding: utf-8 -*-
    """Sentiment Analysis Anies Baswedan_TF-IDF, NB, SVM.ipynb

    Automatically generated by Colaboratory.

    Original file is located at
        https://colab.research.google.com/drive/1cQsWcB2sX-MCyFSriMjqvEdcz0SIck_0

    # About

    Sentiment analysis pada dataset twitter Anies Baswedan menggunakan TF-IDF, Naive Bayes, dan Support Vector Machine
    """

    # from google.colab import drive
    # drive.mount('/content/drive')

    import numpy as np
    import pandas as pd

    """# Dataset"""

    # dataset_ab = 'data/Anies_Labeling_3000.csv'
    dataset_ab = 'static/datasets_anies.csv'

    data_anies = pd.read_csv(dataset_ab, delimiter=';')
    data_anies

    data_anies.isnull().sum()

    df_anies = data_anies.copy()

    """mapping label yang berbentuk angka menjadi bentuk klasifikasi (positif, negatif, neutral)"""

    df_anies["observasi"] = df_anies["label"].map({0: "negatif", 1: "positif", 2: "neutral"})
    df_anies.head()

    # Cek sample dari data
    sample_anies = df_anies.groupby('label').apply(lambda x: x.sample(5))
    sample_anies

    df_anies["observasi"].value_counts()

    """Hitung proporsi distribusi untuk menentukan metode OVR atau OVO dalam model Naive Bayes"""

    # Tentukan distribusi masing-masing hasil labeling
    class_distribution = pd.DataFrame({'sentimen': ['positif', 'negatif', 'neutral'],
                                    'value_counts': [1398, 1119, 483]})

    # Hitung proporsi distribusi
    class_distribution['proporsi'] = (class_distribution['value_counts'] / class_distribution['value_counts'].sum()) * 100
    print(class_distribution)

    if args == 'nb':
        """# TF IDF dan Naive Bayes"""

        def feature_extraction(data, method="tfidf"):
            from sklearn.feature_extraction.text import TfidfVectorizer

            feature_extraction = TfidfVectorizer(sublinear_tf=True)
            features = feature_extraction.fit_transform(data)

            return features

        def nb_classifier(features, label, classifier="naive_bayes"):
            from sklearn.metrics import roc_auc_score
            from sklearn.naive_bayes import MultinomialNB
            from sklearn.preprocessing import label_binarize

            model = MultinomialNB()
            model.fit(features, label)
            probability_to_be_positive = model.predict_proba(features)
            predict_output = model.predict(features)

            # Konversi label multi-class ke format one-hot-encoding
            binarized_label = label_binarize(label, classes=np.unique(label))

            # Gunakan  multi_class='ovo' karena proporsi distribusi tidak seimbang
            print("auc (train data):" , roc_auc_score(
                binarized_label,
                probability_to_be_positive,
                multi_class='ovo'))

            print("top 10 scores:")
            for i in range(10):
                if predict_output[i] == 0:
                    label = 'negatif'
                elif predict_output[i] == 1:
                    label = 'positif'
                elif predict_output[i] == 2:
                    label = 'neutral'
                print(f"{probability_to_be_positive[i]} = {predict_output[i]} ({label})")

            return model

        data = np.array(df_anies["tweet"])
        label = np.array(df_anies["label"])

        """Lakukan pembobotan untuk masing-masing kata pada tweet."""

        features = feature_extraction(data, method="tfidf")
        print(features)

        """Lakukan training menggunakan model Naive Bayes."""

        nb_train = nb_classifier(features, label, "naive_bayes")
        print(nb_train)

        import pandas as pd
        import numpy as np
        import pickle

        from sklearn.naive_bayes import MultinomialNB
        from sklearn.metrics import classification_report
        from sklearn.model_selection import train_test_split
        from sklearn.feature_extraction.text import CountVectorizer
        from sklearn.feature_extraction.text import TfidfTransformer
        from sklearn.pipeline import Pipeline

        """Lakukan pemrosesan TF-IDF dengan data tweet sebagai variabel X dan data label sebagai variabel y."""

        X = df_anies['tweet']
        y = df_anies['label']

        bow_transformer = CountVectorizer().fit(df_anies['tweet'])
        messages_bow = bow_transformer.transform(df_anies['tweet'])
        tfidf_transformer = TfidfTransformer().fit(messages_bow)
        messages_tfidf = tfidf_transformer.transform(messages_bow)

        """Lakukan kembali proses train dan split dengan rasio yang sama yaitu data train sebesar 80% dan data test sebesar 20%."""

        X_train,X_test,y_train,y_test = train_test_split(df_anies['tweet'],df_anies['label'],test_size=0.2,random_state=42)

        pipeline = Pipeline([
            ('bow',CountVectorizer()),
            ('tfidf',TfidfTransformer()),
            ('classifier',MultinomialNB())
            ])

        X_train = np.asarray(X)
        pipeline = pipeline.fit(X_train, np.asarray(y))

        """Buat data pickle setelah proses training selesai dilakukan. Pickle akan digunakan untuk menjalankan proses deep learning."""

        pickle_name = 'df_anies_nb.pickle'
        pickle_path = 'data/'

        with open(pickle_path+pickle_name, 'wb') as f:
            pickle.dump(pipeline, f)

        """Ganti parameter open untuk membaca atau menggunakan pickle."""

        with open(pickle_path+pickle_name, 'rb') as f:
            pickle_anies = pickle.load(f)
            print(pickle_anies)

        """Lakukan prediksi dengan dataset lain"""

        path_test = 'data/Anies_Testing_Model.csv'

        df_test = pd.read_csv(path_test, delimiter=',')
        df_test.head()

        """Berdasarkan dataset, data yang diperlukan pada feature enineering ini hanyalah data terakhir yaitu setelah pemrosesan pada tahap pre-processing selesai dilakukan."""

        df_test = df_test[["stemming"]].rename(columns={"stemming": "tweet"})
        df_test = df_test["tweet"]
        ____df_test = df_test.iloc[:1]
        ____df_test["tweet"] = text
        ____df_test = ____df_test
        df_test

        # null_values = df_test.isnull().sum()
        # print(f"Total null values: {null_values}")

        # duplikat = df_test.duplicated(keep=False).sum()
        # print("Total duplikat:", duplikat)

        # # Drop duplikat jika ada
        # df_test = df_test.drop_duplicates()

        # duplikat = df_test.duplicated(keep=False).sum()
        # print("Total duplikat:", duplikat)

        """Lakukan prediksi menggunakan pickle yang telah dibuat sebelumnya."""

        prediction = pickle_anies.predict(np.asarray(____df_test))
        prediction

        """Simpan hasil prediksi ke dalam dataset dengan cara melakukan looping sesuai hasil prediksi. Parameter yang digunakan masih sama seperti sebelumnya, yaitu:

        - Prediksi 0: Negatif
        - Prediksi 1: Positif
        - Prediksi 2: Neutral
        """

        result = []

        for i in range(len(prediction)):
            if(prediction[i] == 0):
                sentiment = 'negatif'
            elif(prediction[i] == 1):
                sentiment = 'positif'
            elif(prediction[i] == 2):
                sentiment = 'neutral'

            result.append({'tweet':____df_test.iloc[i],'label':prediction[i],'prediksi':sentiment})

        return result
    else:
        """# TF IDF dan Support Vector Machine"""

        def feature_extraction(data, method="tfidf"):
            from sklearn.feature_extraction.text import TfidfVectorizer

            feature_extraction = TfidfVectorizer(sublinear_tf=True)
            features = feature_extraction.fit_transform(data)

            return features

        def svm_classifier(features, label, classifier="svm"):
            from sklearn.metrics import roc_auc_score
            from sklearn.svm import SVC
            from sklearn.preprocessing import label_binarize

            model = SVC(probability=True)
            model.fit(features, label)
            probability_to_be_positive = model.predict_proba(features)
            predict_output = model.predict(features)

            # Konversi label multi-class ke format one-hot-encoding
            binarized_label = label_binarize(label, classes=np.unique(label))

            # Gunakan  multi_class='ovo' karena proporsi distribusi tidak seimbang
            print("auc (train data):" , roc_auc_score(
                binarized_label,
                probability_to_be_positive,
                multi_class='ovo'))

            print("top 10 scores:")
            for i in range(10):
                if predict_output[i] == 0:
                    label = 'negatif'
                elif predict_output[i] == 1:
                    label = 'positif'
                elif predict_output[i] == 2:
                    label = 'neutral'
                print(f"{probability_to_be_positive[i]} = {predict_output[i]} ({label})")

            return model

        data = np.array(df_anies["tweet"])
        label = np.array(df_anies["label"])

        """Lakukan pembobotan untuk masing-masing kata pada tweet."""

        features = feature_extraction(data, method="tfidf")
        print(features)

        """Lakukan training menggunakan model Naive Bayes."""

        svm_train = svm_classifier(features, label, "naive_bayes")
        print(svm_train)

        import pandas as pd
        import numpy as np
        import pickle

        from sklearn.svm import SVC
        from sklearn.metrics import classification_report
        from sklearn.model_selection import train_test_split
        from sklearn.feature_extraction.text import CountVectorizer
        from sklearn.feature_extraction.text import TfidfTransformer
        from sklearn.pipeline import Pipeline

        """Lakukan pemrosesan TF-IDF dengan data tweet sebagai variabel X dan data label sebagai variabel y."""

        X = df_anies['tweet']
        y = df_anies['label']

        bow_transformer = CountVectorizer().fit(df_anies['tweet'])
        messages_bow = bow_transformer.transform(df_anies['tweet'])
        tfidf_transformer = TfidfTransformer().fit(messages_bow)
        messages_tfidf = tfidf_transformer.transform(messages_bow)

        """Lakukan kembali proses train dan split dengan rasio yang sama yaitu data train sebesar 80% dan data test sebesar 20%."""

        X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)

        pipeline = Pipeline([
            ('bow',CountVectorizer()),
            ('tfidf',TfidfTransformer()),
            ('classifier',SVC())
            ])

        X_train = np.asarray(X)
        pipeline = pipeline.fit(X_train, np.asarray(y))

        """Buat data pickle setelah proses training selesai dilakukan. Pickle akan digunakan untuk menjalankan proses deep learning."""

        pickle_name = 'df_anies_svm.pickle'
        pickle_path = 'data/'

        with open(pickle_path+pickle_name, 'wb') as f:
            pickle.dump(pipeline, f)

        """Ganti parameter open untuk membaca atau menggunakan pickle."""

        with open(pickle_path+pickle_name, 'rb') as f:
            pickle_anies_svm = pickle.load(f)
            print(pickle_anies_svm)

        """Lakukan prediksi dengan dataset lain"""

        path_test = 'data/Anies_Testing_Model.csv'

        df_test = pd.read_csv(path_test, delimiter=',')
        df_test.head()

        """Berdasarkan dataset, data yang diperlukan pada feature enineering ini hanyalah data terakhir yaitu setelah pemrosesan pada tahap pre-processing selesai dilakukan."""

        df_test = df_test[["stemming"]].rename(columns={"stemming": "tweet"})
        df_test = df_test["tweet"]
        ___df_test = df_test.iloc[:1]
        ___df_test["tweet"] = text
        ___df_test = ___df_test
        df_test

        # null_values = df_test.isnull().sum()
        # print(f"Total null values: {null_values}")

        # duplikat = df_test.duplicated(keep=False).sum()
        # print("Total duplikat:", duplikat)

        # # Drop duplikat jika ada
        # df_test = df_test.drop_duplicates()

        # duplikat = df_test.duplicated(keep=False).sum()
        # print("Total duplikat:", duplikat)

        # """Lakukan prediksi menggunakan pickle yang telah dibuat sebelumnya."""

        # prediction = pickle_anies_svm.predict(text)
        # return prediction
        
        prediction = pickle_anies_svm.predict(np.asarray(___df_test))
        prediction

        """Simpan hasil prediksi ke dalam dataset dengan cara melakukan looping sesuai hasil prediksi. Parameter yang digunakan masih sama seperti sebelumnya, yaitu:

        - Prediksi 0: Negatif
        - Prediksi 1: Positif
        - Prediksi 2: Neutral
        """

        result = []

        for i in range(len(prediction)):
            if(prediction[i] == 0):
                sentiment = 'negatif'
            elif(prediction[i] == 1):
                sentiment = 'positif'
            elif(prediction[i] == 2):
                sentiment = 'neutral'

            result.append({'tweet':___df_test.iloc[i],'label':prediction[i],'prediksi':sentiment})

        return result
        # data = pd.DataFrame(result)
        # data

def _PrediksiCsv(args):
    # -*- coding: utf-8 -*-
    """Sentiment Analysis Anies Baswedan_TF-IDF, NB, SVM.ipynb

    Automatically generated by Colaboratory.

    Original file is located at
        https://colab.research.google.com/drive/1cQsWcB2sX-MCyFSriMjqvEdcz0SIck_0

    # About

    Sentiment analysis pada dataset twitter Anies Baswedan menggunakan TF-IDF, Naive Bayes, dan Support Vector Machine
    """

    # from google.colab import drive
    # drive.mount('/content/drive')

    import numpy as np
    import pandas as pd

    """# Dataset"""

    # dataset_ab = 'data/Anies_Labeling_3000.csv'
    dataset_ab = 'static/datasets_anies.csv'

    data_anies = pd.read_csv(dataset_ab, delimiter=';')
    data_anies

    data_anies.isnull().sum()

    df_anies = data_anies.copy()

    """mapping label yang berbentuk angka menjadi bentuk klasifikasi (positif, negatif, neutral)"""

    df_anies["observasi"] = df_anies["label"].map({0: "negatif", 1: "positif", 2: "neutral"})
    df_anies.head()

    # Cek sample dari data
    sample_anies = df_anies.groupby('label').apply(lambda x: x.sample(5))
    sample_anies

    df_anies["observasi"].value_counts()

    """Hitung proporsi distribusi untuk menentukan metode OVR atau OVO dalam model Naive Bayes"""

    # Tentukan distribusi masing-masing hasil labeling
    class_distribution = pd.DataFrame({'sentimen': ['positif', 'negatif', 'neutral'],
                                    'value_counts': [1398, 1119, 483]})

    # Hitung proporsi distribusi
    class_distribution['proporsi'] = (class_distribution['value_counts'] / class_distribution['value_counts'].sum()) * 100
    print(class_distribution)

    def casefolding(tweet):#function
        tweet = tweet.lower()
        return tweet
    def cleaning(tweet):#function
        import re
        tweet = re.sub(r'https\S+|www\S+https\S+', '',tweet, flags=re.MULTILINE) #link
        tweet = re.sub(r'@\w+|#\w+', '', tweet) #mention atau username
        tweet = re.sub(r"\b[a-zA-Z]\b", '', tweet) #remove single char
        tweet = re.sub(r'[^\w\s]', '', tweet) #spasi
        tweet = re.sub(r"[0-9]+",'',tweet) #angka
        tweet = re.sub(r"[\n]", ' ' ,tweet) #enter
        tweet = re.sub(r'[\s]+', ' ',tweet) #double whitespace ''
        return tweet
    # import word_tokenize dari library nltk
    import nltk
    from nltk.tokenize import word_tokenize
    def tokenizing(tweet):
        tweet_tokens = word_tokenize(tweet)
        return tweet_tokens
    from nltk.corpus import stopwords
    def stopwordRemoval(tweet):
        tweet_tokens = word_tokenize(tweet)
        stop_words = set(stopwords.words('indonesian'))
        filtered_tweet = [w for w in tweet_tokens if not w in stop_words]
        return filtered_tweet
    from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
    fact = StemmerFactory()
    stemmer = fact.create_stemmer()
    
    import csv
    reader_normalization = csv.reader(open(r"data/normalisasi.csv", "r"))
    dict_normalization = {row[0]:row[1] for row in reader_normalization if row and row[0]}

    def normalized_term(document):
        return [dict_normalization[term] if term in dict_normalization else term for term in document]
    
    def stemming(data):
        # tweet = [stemmer.stem(word) for word in data]
        # return ' '.join(tweet)
        return ' '.join(data)

    import nltk
    from nltk.tokenize import word_tokenize
    def tokenizing(tweet):
        tweet_tokens = word_tokenize(tweet)
        return tweet_tokens

    if args == 'nb':
        """# TF IDF dan Naive Bayes"""

        def feature_extraction(data, method="tfidf"):
            from sklearn.feature_extraction.text import TfidfVectorizer

            feature_extraction = TfidfVectorizer(sublinear_tf=True)
            features = feature_extraction.fit_transform(data)

            return features

        def nb_classifier(features, label, classifier="naive_bayes"):
            from sklearn.metrics import roc_auc_score
            from sklearn.naive_bayes import MultinomialNB
            from sklearn.preprocessing import label_binarize

            model = MultinomialNB()
            model.fit(features, label)
            probability_to_be_positive = model.predict_proba(features)
            predict_output = model.predict(features)

            # Konversi label multi-class ke format one-hot-encoding
            binarized_label = label_binarize(label, classes=np.unique(label))

            # Gunakan  multi_class='ovo' karena proporsi distribusi tidak seimbang
            print("auc (train data):" , roc_auc_score(
                binarized_label,
                probability_to_be_positive,
                multi_class='ovo'))

            print("top 10 scores:")
            for i in range(10):
                if predict_output[i] == 0:
                    label = 'negatif'
                elif predict_output[i] == 1:
                    label = 'positif'
                elif predict_output[i] == 2:
                    label = 'neutral'
                print(f"{probability_to_be_positive[i]} = {predict_output[i]} ({label})")

            return model

        data = np.array(df_anies["tweet"])
        label = np.array(df_anies["label"])

        """Lakukan pembobotan untuk masing-masing kata pada tweet."""

        features = feature_extraction(data, method="tfidf")
        print(features)

        """Lakukan training menggunakan model Naive Bayes."""

        nb_train = nb_classifier(features, label, "naive_bayes")
        print(nb_train)

        import pandas as pd
        import numpy as np
        import pickle

        from sklearn.naive_bayes import MultinomialNB
        from sklearn.metrics import classification_report
        from sklearn.model_selection import train_test_split
        from sklearn.feature_extraction.text import CountVectorizer
        from sklearn.feature_extraction.text import TfidfTransformer
        from sklearn.pipeline import Pipeline

        """Lakukan pemrosesan TF-IDF dengan data tweet sebagai variabel X dan data label sebagai variabel y."""

        X = df_anies['tweet']
        y = df_anies['label']

        bow_transformer = CountVectorizer().fit(df_anies['tweet'])
        messages_bow = bow_transformer.transform(df_anies['tweet'])
        tfidf_transformer = TfidfTransformer().fit(messages_bow)
        messages_tfidf = tfidf_transformer.transform(messages_bow)

        """Lakukan kembali proses train dan split dengan rasio yang sama yaitu data train sebesar 80% dan data test sebesar 20%."""

        X_train,X_test,y_train,y_test = train_test_split(df_anies['tweet'],df_anies['label'],test_size=0.2,random_state=42)

        pipeline = Pipeline([
            ('bow',CountVectorizer()),
            ('tfidf',TfidfTransformer()),
            ('classifier',MultinomialNB())
            ])

        X_train = np.asarray(X)
        pipeline = pipeline.fit(X_train, np.asarray(y))

        """Buat data pickle setelah proses training selesai dilakukan. Pickle akan digunakan untuk menjalankan proses deep learning."""

        pickle_name = 'df_anies_nb.pickle'
        pickle_path = 'data/'

        with open(pickle_path+pickle_name, 'wb') as f:
            pickle.dump(pipeline, f)

        """Ganti parameter open untuk membaca atau menggunakan pickle."""

        with open(pickle_path+pickle_name, 'rb') as f:
            pickle_anies = pickle.load(f)
            print(pickle_anies)

        """Lakukan prediksi dengan dataset lain"""

        path_test = 'data/datasets_upload.csv'

        df_test = pd.read_csv(path_test, delimiter=',')
        df_test.head()

        """Berdasarkan dataset, data yang diperlukan pada feature enineering ini hanyalah data terakhir yaitu setelah pemrosesan pada tahap pre-processing selesai dilakukan."""

        df1_test = df_test["tweet"]

        df_test['tweet'] = df_test['tweet'].apply(casefolding)
        df_test['tweet'] = df_test['tweet'].apply(cleaning)
        df_test['tweet'] = df_test['tweet'].apply(tokenizing)
        df_test['tweet'] = df_test['tweet'].apply(normalized_term)
        df_test['tweet'] = df_test['tweet'].str.join(' ')
        df_test['tweet'] = df_test['tweet'].apply(stopwordRemoval)
        df_test['tweet'] = df_test['tweet'].apply(stemming)
        df_test = df_test["tweet"]
        df_test

        """Lakukan prediksi menggunakan pickle yang telah dibuat sebelumnya."""

        prediction = pickle_anies.predict(np.asarray(df_test))
        prediction

        """Simpan hasil prediksi ke dalam dataset dengan cara melakukan looping sesuai hasil prediksi. Parameter yang digunakan masih sama seperti sebelumnya, yaitu:

        - Prediksi 0: Negatif
        - Prediksi 1: Positif
        - Prediksi 2: Neutral
        """

        result = []

        for i in range(len(prediction)):
            if(prediction[i] == 0):
                sentiment = 'negatif'
            elif(prediction[i] == 1):
                sentiment = 'positif'
            elif(prediction[i] == 2):
                sentiment = 'neutral'

            result.append({'tweet':df1_test.iloc[i],'label':prediction[i],'prediksi':sentiment})

        import os
        os.unlink("static/hasil_prediksi_naive_bayes.csv")

        _data = pd.DataFrame(result)
        _data.to_csv("static/hasil_prediksi_naive_bayes.csv")
        return result
    else:
        """# TF IDF dan Support Vector Machine"""

        def feature_extraction(data, method="tfidf"):
            from sklearn.feature_extraction.text import TfidfVectorizer

            feature_extraction = TfidfVectorizer(sublinear_tf=True)
            features = feature_extraction.fit_transform(data)

            return features

        def svm_classifier(features, label, classifier="svm"):
            from sklearn.metrics import roc_auc_score
            from sklearn.svm import SVC
            from sklearn.preprocessing import label_binarize

            model = SVC(probability=True)
            model.fit(features, label)
            probability_to_be_positive = model.predict_proba(features)
            predict_output = model.predict(features)

            # Konversi label multi-class ke format one-hot-encoding
            binarized_label = label_binarize(label, classes=np.unique(label))

            # Gunakan  multi_class='ovo' karena proporsi distribusi tidak seimbang
            print("auc (train data):" , roc_auc_score(
                binarized_label,
                probability_to_be_positive,
                multi_class='ovo'))

            print("top 10 scores:")
            for i in range(10):
                if predict_output[i] == 0:
                    label = 'negatif'
                elif predict_output[i] == 1:
                    label = 'positif'
                elif predict_output[i] == 2:
                    label = 'neutral'
                print(f"{probability_to_be_positive[i]} = {predict_output[i]} ({label})")

            return model

        data = np.array(df_anies["tweet"])
        label = np.array(df_anies["label"])

        """Lakukan pembobotan untuk masing-masing kata pada tweet."""

        features = feature_extraction(data, method="tfidf")
        print(features)

        """Lakukan training menggunakan model Naive Bayes."""

        svm_train = svm_classifier(features, label, "naive_bayes")
        print(svm_train)

        import pandas as pd
        import numpy as np
        import pickle

        from sklearn.svm import SVC
        from sklearn.metrics import classification_report
        from sklearn.model_selection import train_test_split
        from sklearn.feature_extraction.text import CountVectorizer
        from sklearn.feature_extraction.text import TfidfTransformer
        from sklearn.pipeline import Pipeline

        """Lakukan pemrosesan TF-IDF dengan data tweet sebagai variabel X dan data label sebagai variabel y."""

        X = df_anies['tweet']
        y = df_anies['label']

        bow_transformer = CountVectorizer().fit(df_anies['tweet'])
        messages_bow = bow_transformer.transform(df_anies['tweet'])
        tfidf_transformer = TfidfTransformer().fit(messages_bow)
        messages_tfidf = tfidf_transformer.transform(messages_bow)

        """Lakukan kembali proses train dan split dengan rasio yang sama yaitu data train sebesar 80% dan data test sebesar 20%."""

        X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)

        pipeline = Pipeline([
            ('bow',CountVectorizer()),
            ('tfidf',TfidfTransformer()),
            ('classifier',SVC())
            ])

        X_train = np.asarray(X)
        pipeline = pipeline.fit(X_train, np.asarray(y))

        """Buat data pickle setelah proses training selesai dilakukan. Pickle akan digunakan untuk menjalankan proses deep learning."""

        pickle_name = 'df_anies_svm.pickle'
        pickle_path = 'data/'

        with open(pickle_path+pickle_name, 'wb') as f:
            pickle.dump(pipeline, f)

        """Ganti parameter open untuk membaca atau menggunakan pickle."""

        with open(pickle_path+pickle_name, 'rb') as f:
            pickle_anies_svm = pickle.load(f)
            print(pickle_anies_svm)

        """Lakukan prediksi dengan dataset lain"""
        path_test = 'data/datasets_upload.csv'

        df_test = pd.read_csv(path_test, delimiter=',')
        df_test.head()

        """Berdasarkan dataset, data yang diperlukan pada feature enineering ini hanyalah data terakhir yaitu setelah pemrosesan pada tahap pre-processing selesai dilakukan."""

        df1_test = df_test["tweet"]

        df_test['tweet'] = df_test['tweet'].apply(casefolding)
        df_test['tweet'] = df_test['tweet'].apply(cleaning)
        df_test['tweet'] = df_test['tweet'].apply(tokenizing)
        df_test['tweet'] = df_test['tweet'].apply(normalized_term)
        df_test['tweet'] = df_test['tweet'].str.join(' ')
        df_test['tweet'] = df_test['tweet'].apply(stopwordRemoval)
        df_test['tweet'] = df_test['tweet'].apply(stemming)
        df_test = df_test["tweet"]
        df_test
        
        prediction = pickle_anies_svm.predict(np.asarray(df_test))
        prediction

        """Simpan hasil prediksi ke dalam dataset dengan cara melakukan looping sesuai hasil prediksi. Parameter yang digunakan masih sama seperti sebelumnya, yaitu:

        - Prediksi 0: Negatif
        - Prediksi 1: Positif
        - Prediksi 2: Neutral
        """

        result = []

        for i in range(len(prediction)):
            if(prediction[i] == 0):
                sentiment = 'negatif'
            elif(prediction[i] == 1):
                sentiment = 'positif'
            elif(prediction[i] == 2):
                sentiment = 'neutral'

            result.append({'tweet':df1_test.iloc[i],'label':prediction[i],'prediksi':sentiment})

        import os
        os.unlink("static/hasil_prediksi_svm.csv")

        _data = pd.DataFrame(result)
        _data.to_csv("static/hasil_prediksi_svm.csv")
        return result
        # data = pd.DataFrame(result)
        # data

def _UpdateModelAnies():
    import numpy as np
    import pandas as pd

    def casefolding(tweet):#function
        tweet = tweet.lower()
        return tweet
    def cleaning(tweet):#function
        import re
        tweet = re.sub(r'https\S+|www\S+https\S+', '',tweet, flags=re.MULTILINE) #link
        tweet = re.sub(r'@\w+|#\w+', '', tweet) #mention atau username
        tweet = re.sub(r"\b[a-zA-Z]\b", '', tweet) #remove single char
        tweet = re.sub(r'[^\w\s]', '', tweet) #spasi
        tweet = re.sub(r"[0-9]+",'',tweet) #angka
        tweet = re.sub(r"[\n]", ' ' ,tweet) #enter
        tweet = re.sub(r'[\s]+', ' ',tweet) #double whitespace ''
        return tweet
    # import word_tokenize dari library nltk
    import nltk
    from nltk.tokenize import word_tokenize
    def tokenizing(tweet):
        tweet_tokens = word_tokenize(tweet)
        return tweet_tokens
    from nltk.corpus import stopwords
    def stopwordRemoval(tweet):
        tweet_tokens = word_tokenize(tweet)
        stop_words = set(stopwords.words('indonesian'))
        filtered_tweet = [w for w in tweet_tokens if not w in stop_words]
        return filtered_tweet
    from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
    fact = StemmerFactory()
    stemmer = fact.create_stemmer()
    
    import csv
    reader_normalization = csv.reader(open(r"data/normalisasi.csv", "r"))
    dict_normalization = {row[0]:row[1] for row in reader_normalization if row and row[0]}

    def normalized_term(document):
        return [dict_normalization[term] if term in dict_normalization else term for term in document]
    
    def stemming(data):
        # tweet = [stemmer.stem(word) for word in data]
        # return ' '.join(tweet)
        return ' '.join(data)

    import nltk
    from nltk.tokenize import word_tokenize
    def tokenizing(tweet):
        tweet_tokens = word_tokenize(tweet)
        return tweet_tokens
    
    def sentiment(label):
        if label < 0:
            return 0
        elif 0 <= label < 1:
            return 1
        else:
            return 2
    
    dataset_ab = 'static/datasets_anies_upload.csv'
    data_anies = pd.read_csv(dataset_ab, encoding="utf8")
    
    # data_anies['tweet'] = data_anies.apply(lambda row: row['tweet'] if row['label'] else stemming(stopwordRemoval(normalized_term(tokenizing(cleaning(casefolding(row['tweet'])))))), axis=1)
    data_anies['tweet'] = data_anies['tweet'].apply(casefolding)
    data_anies['tweet'] = data_anies['tweet'].apply(cleaning)
    data_anies['tweet'] = data_anies['tweet'].apply(tokenizing)
    data_anies['tweet'] = data_anies['tweet'].apply(normalized_term)
    data_anies['tweet'] = data_anies['tweet'].str.join(' ')
    data_anies['tweet'] = data_anies['tweet'].apply(stopwordRemoval)
    label = pd.DataFrame(data_anies['tweet'].str.join(' '))

    data_anies['tweet'] = data_anies['tweet'].apply(stemming)

    tweet_tokens = word_tokenize
    tweettoken = [tweet_tokens(i) for i in label['tweet']]
    label['tweet token'] = tweettoken
    label['length'] = label['tweet token'].map(lambda x: len(x))

    filenegative = 'static/negative.txt'
    filepositive = 'static/positive.txt'

    file = open(filenegative, 'r')
    neg_words = file.read().split()
    file = open(filepositive, 'r')
    pos_words = file.read().split()

    num_pos = label['tweet token'].map(lambda x: len([i for i in x if i in pos_words]))
    label['pos_count'] = num_pos
    num_neg = label['tweet token'].map(lambda x: len([i for i in x if i in neg_words]))
    label['neg_count'] = num_neg

    label['StSc'] = (label['pos_count'] - label['neg_count']) / label['length']
    label = label.drop(['tweet token'], axis = 1)
        
    label['label'] = label['StSc'].apply(sentiment)
    label = label.drop(['length', 'pos_count', 'neg_count'], axis = 1)
    
    data_anies = pd.concat([data_anies.tweet, label.label], axis=1)

    dataset_old = 'static/datasets_anies.csv'
    data_anies_old = pd.read_csv(dataset_old, delimiter=';')
    res = pd.concat([data_anies_old, data_anies])
    res = res[res['tweet'].notnull() & (res['tweet'] != '')]
    # res.dropna(subset=['tweet'])
    # res.reset_index(drop=True)

    import os
    os.unlink("static/datasets_anies.csv")
    res.to_csv("static/datasets_anies.csv", sep=';', index=False)
    
    return True

def _UpdateModelGanjar():
    import numpy as np
    import pandas as pd

    def casefolding(tweet):#function
        tweet = tweet.lower()
        return tweet
    def cleaning(tweet):#function
        import re
        tweet = re.sub(r'https\S+|www\S+https\S+', '',tweet, flags=re.MULTILINE) #link
        tweet = re.sub(r'@\w+|#\w+', '', tweet) #mention atau username
        tweet = re.sub(r"\b[a-zA-Z]\b", '', tweet) #remove single char
        tweet = re.sub(r'[^\w\s]', '', tweet) #spasi
        tweet = re.sub(r"[0-9]+",'',tweet) #angka
        tweet = re.sub(r"[\n]", ' ' ,tweet) #enter
        tweet = re.sub(r'[\s]+', ' ',tweet) #double whitespace ''
        return tweet
    # import word_tokenize dari library nltk
    import nltk
    from nltk.tokenize import word_tokenize
    def tokenizing(tweet):
        tweet_tokens = word_tokenize(tweet)
        return tweet_tokens
    from nltk.corpus import stopwords
    def stopwordRemoval(tweet):
        tweet_tokens = word_tokenize(tweet)
        stop_words = set(stopwords.words('indonesian'))
        filtered_tweet = [w for w in tweet_tokens if not w in stop_words]
        return filtered_tweet
    from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
    fact = StemmerFactory()
    stemmer = fact.create_stemmer()
    
    import csv
    reader_normalization = csv.reader(open(r"data/normalisasi.csv", "r"))
    dict_normalization = {row[0]:row[1] for row in reader_normalization if row and row[0]}

    def normalized_term(document):
        return [dict_normalization[term] if term in dict_normalization else term for term in document]
    
    def stemming(data):
        # tweet = [stemmer.stem(word) for word in data]
        # return ' '.join(tweet)
        return ' '.join(data)

    import nltk
    from nltk.tokenize import word_tokenize
    def tokenizing(tweet):
        tweet_tokens = word_tokenize(tweet)
        return tweet_tokens
    
    def sentiment(label):
        if label < 0:
            return 0
        elif 0 <= label < 1:
            return 1
        else:
            return 2
    
    dataset_ab = 'static/datasets_ganjar_upload.csv'
    data_anies = pd.read_csv(dataset_ab, encoding="utf8")
    
    # data_anies['tweet'] = data_anies.apply(lambda row: row['tweet'] if row['label'] else stemming(stopwordRemoval(normalized_term(tokenizing(cleaning(casefolding(row['tweet'])))))), axis=1)
    data_anies['tweet'] = data_anies['tweet'].apply(casefolding)
    data_anies['tweet'] = data_anies['tweet'].apply(cleaning)
    data_anies['tweet'] = data_anies['tweet'].apply(tokenizing)
    data_anies['tweet'] = data_anies['tweet'].apply(normalized_term)
    data_anies['tweet'] = data_anies['tweet'].str.join(' ')
    data_anies['tweet'] = data_anies['tweet'].apply(stopwordRemoval)
    label = pd.DataFrame(data_anies['tweet'].str.join(' '))

    data_anies['tweet'] = data_anies['tweet'].apply(stemming)

    tweet_tokens = word_tokenize
    tweettoken = [tweet_tokens(i) for i in label['tweet']]
    label['tweet token'] = tweettoken
    label['length'] = label['tweet token'].map(lambda x: len(x))

    filenegative = 'static/negative.txt'
    filepositive = 'static/positive.txt'

    file = open(filenegative, 'r')
    neg_words = file.read().split()
    file = open(filepositive, 'r')
    pos_words = file.read().split()

    num_pos = label['tweet token'].map(lambda x: len([i for i in x if i in pos_words]))
    label['pos_count'] = num_pos
    num_neg = label['tweet token'].map(lambda x: len([i for i in x if i in neg_words]))
    label['neg_count'] = num_neg

    label['StSc'] = (label['pos_count'] - label['neg_count']) / label['length']
    label = label.drop(['tweet token'], axis = 1)
        
    label['label'] = label['StSc'].apply(sentiment)
    label = label.drop(['length', 'pos_count', 'neg_count'], axis = 1)
    
    data_anies = pd.concat([data_anies.tweet, label.label], axis=1)

    dataset_old = 'static/datasets_ganjar.csv'
    data_anies_old = pd.read_csv(dataset_old, delimiter=';')
    res = pd.concat([data_anies_old, data_anies])
    res = res[res['tweet'].notnull() & (res['tweet'] != '')]
    # res.dropna(subset=['tweet'])
    # res.reset_index(drop=True)

    import os
    os.unlink("static/datasets_ganjar.csv")
    res.to_csv("static/datasets_ganjar.csv", sep=';', index=False)
    
    return True


class NumpyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return json.JSONEncoder.default(self, obj)

try:
    from home.models import Product

except:
    pass

class ProductView(APIView):

    permission_classes = (IsAuthenticatedOrReadOnly,)

    def post(self, request):
        serializer = ProductSerializer(data=request.data)
        if not serializer.is_valid():
            return Response(data={
                **serializer.errors,
                'success': False
            }, status=HTTPStatus.BAD_REQUEST)
        serializer.save()
        return Response(data={
            'message': 'Record Created.',
            'success': True
        }, status=HTTPStatus.OK)

    def get(self, request, pk=None):
        if not pk:
            return Response({
                'data': [ProductSerializer(instance=obj).data for obj in Product.objects.all()],
                'success': True
            }, status=HTTPStatus.OK)
        try:
            obj = get_object_or_404(Product, pk=pk)
        except Http404:
            return Response(data={
                'message': 'object with given id not found.',
                'success': False
            }, status=HTTPStatus.NOT_FOUND)
        return Response({
            'data': ProductSerializer(instance=obj).data,
            'success': True
        }, status=HTTPStatus.OK)

    def put(self, request, pk):
        try:
            obj = get_object_or_404(Product, pk=pk)
        except Http404:
            return Response(data={
                'message': 'object with given id not found.',
                'success': False
            }, status=HTTPStatus.NOT_FOUND)
        serializer = ProductSerializer(instance=obj, data=request.data, partial=True)
        if not serializer.is_valid():
            return Response(data={
                **serializer.errors,
                'success': False
            }, status=HTTPStatus.BAD_REQUEST)
        serializer.save()
        return Response(data={
            'message': 'Record Updated.',
            'success': True
        }, status=HTTPStatus.OK)

    def delete(self, request, pk):
        try:
            obj = get_object_or_404(Product, pk=pk)
        except Http404:
            return Response(data={
                'message': 'object with given id not found.',
                'success': False
            }, status=HTTPStatus.NOT_FOUND)
        obj.delete()
        return Response(data={
            'message': 'Record Deleted.',
            'success': True
        }, status=HTTPStatus.OK)

class AniesBaswedan(APIView):

    permission_classes = (IsAuthenticatedOrReadOnly,)

    def get(self, request, pk=None):
        if where_json('data/anies_baswedan.json'):
            f = open('data/anies_baswedan.json')
            _result = json.load(f)
        else:
            result = DataAniesBaswedan()
            _result = {
                "n_neutral": result[0],
                "n_positif": result[1],
                "n_negatif": result[2],
                "data_neutral": result[3],
                "data_positif": result[4],
                "data_negatif": result[5],
                "cvs": result[6]
            }
            with open("data/anies_baswedan.json", "w") as outfile:
                json.dump(_result, outfile, cls=NumpyEncoder)

        return Response({
            'data': _result,
            'success': True
        }, status=HTTPStatus.OK)

class GanjarPranowo(APIView):

    permission_classes = (IsAuthenticatedOrReadOnly,)

    def get(self, request, pk=None):
        if where_json('data/ganjar_pranowo.json'):
            f = open('data/ganjar_pranowo.json')
            _result = json.load(f)
        else:
            result = DataGanjarPranowo()
            _result = {
                "n_neutral": result[0],
                "n_positif": result[1],
                "n_negatif": result[2],
                "data_neutral": result[3],
                "data_positif": result[4],
                "data_negatif": result[5],
                "cvs": result[6]
            }
            with open("data/ganjar_pranowo.json", "w") as outfile:
                json.dump(_result, outfile, cls=NumpyEncoder)

        return Response({
            'data': _result,
            'success': True
        }, status=HTTPStatus.OK)

class AniesBaswedanNB(APIView):

    permission_classes = (IsAuthenticatedOrReadOnly,)

    def get(self, request, pk=None):
        if where_json('data/anies_baswedan_nb.json'):
            f = open('data/anies_baswedan_nb.json')
            _result = json.load(f)
        else:
            result = DataAniesBaswedanNB()
            _result = {
                "n_neutral": result[0],
                "n_positif": result[1],
                "n_negatif": result[2],
                "data_neutral": result[3],
                "data_positif": result[4],
                "data_negatif": result[5],
                "cvs": result[6]
            }
            with open("data/anies_baswedan_nb.json", "w") as outfile:
                json.dump(_result, outfile, cls=NumpyEncoder)

        return Response({
            'data': _result,
            'success': True
        }, status=HTTPStatus.OK)

class GanjarPranowoNB(APIView):

    permission_classes = (IsAuthenticatedOrReadOnly,)

    def get(self, request, pk=None):
        if where_json('data/ganjar_pranowo_nb.json'):
            f = open('data/ganjar_pranowo_nb.json')
            _result = json.load(f)
        else:
            result = DataGanjarPranowoNB()
            _result = {
                "n_neutral": result[0],
                "n_positif": result[1],
                "n_negatif": result[2],
                "data_neutral": result[3],
                "data_positif": result[4],
                "data_negatif": result[5],
                "cvs": result[6]
            }
            with open("data/ganjar_pranowo_nb.json", "w") as outfile:
                json.dump(_result, outfile, cls=NumpyEncoder)

        return Response({
            'data': _result,
            'success': True
        }, status=HTTPStatus.OK)

class PreProcessing(APIView):

    # permission_classes = (IsAuthenticatedOrReadOnly,)
    def post(self, request, pk=None):
        text = request.POST.get('text')
        res = _PreProcessing(text)

        return Response({
            'data': res,
            'success': True
        }, status=HTTPStatus.OK)

class Prediksi(APIView):

    # permission_classes = (IsAuthenticatedOrReadOnly,)
    def post(self, request, pk=None):
        text = request.POST.get('text')
        res = _Prediksi(text, 'nb')
        ress = _Prediksi(text, 'svm')

        return Response({
            'data': res,
            'dataa': ress,
            'success': True
        }, status=HTTPStatus.OK)

class PrediksiCsv(APIView):

    # permission_classes = (IsAuthenticatedOrReadOnly,)
    def post(self, request, pk=None):
        text = request.POST.get('text')
        res = _PrediksiCsv('nb')
        ress = _PrediksiCsv('svm')

        return Response({
            'data': [],
            'success': True
        }, status=HTTPStatus.OK)

class UpdateModelAnies(APIView):
    def post(self, request, pk=None):
        res = _UpdateModelAnies()
        return Response({
            'data': [],
            'success': True
        }, status=HTTPStatus.OK)

class UpdateModelGanjar(APIView):
    def post(self, request, pk=None):
        res = _UpdateModelGanjar()
        return Response({
            'data': [],
            'success': True
        }, status=HTTPStatus.OK)
